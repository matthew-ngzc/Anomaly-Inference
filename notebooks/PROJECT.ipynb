{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aedec755",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3dc61b84",
      "metadata": {
        "id": "3dc61b84"
      },
      "source": [
        "# KERNEL LENNEL WEEK 1 CS 421 PROJECT\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ade660",
      "metadata": {
        "id": "38ade660"
      },
      "source": [
        "### 1. Background & Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce15d29",
      "metadata": {
        "id": "4ce15d29"
      },
      "source": [
        "In this project, you will be working with data extracted from famous recommender systems type datasets: you are provided with a large set of interactions between users (persons) and items (movies). Whenever a user \"interacts\" with an item, it watches the movie and gives a \"rating\". There are 5 possible ratings expressed as a \"number of stars\": 1,2,3,4, or 5."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e2fab5e",
      "metadata": {
        "id": "9e2fab5e"
      },
      "source": [
        "In this exercise, we will **not** be performing the recommendation task per se. Instead, you will try to identify the amount of noise/corruption which was injected in each user. Indeed, for each of the users you have been given, an anomaly/noise generation procedure was applied to corrupt the sample. The noise generation procedure depends on two variables: the noise level $p\\in [0,1]$ and the noise type $X\\in\\{0,1,2\\}$.  Each user has been randomly assigned a noise level $p$ and anomaly/noise type $X$, and subsequently been corrupted with the associated noise generation procedure.\n",
        "\n",
        "You have two tasks: first, you must predict the noise level $p$ associated to each test user. This is a **supervised regression task**. Second, you must try to identify the noise generation type for each user. This is a classification task with three classes, with the possibility of including more classes later depending on class performance. This task will be semi-supervised: only a very small number of labels is provided. You will therefore need to combine supervised and unsupervised approaches for this component."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031c248f",
      "metadata": {
        "id": "031c248f"
      },
      "source": [
        "### 2. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64df689c",
      "metadata": {
        "id": "64df689c"
      },
      "source": [
        "You are provided with three frames: the first one (\"X\") contains the interactions provided to you, and the second one (\"yy\") contains the continuous for the users. The third data frame \"yy_cat\" contains the anomaly/noise type for 15 users. The idea is to use these users to disambiguate the category types, but the task will mostly be unsupervised."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fc60513",
      "metadata": {
        "id": "8fc60513"
      },
      "source": [
        "As you can see, the three columns in \"X\" correspond to the user ID, the item ID and the rating (encoded into numerical form). Thus, each row of \"X\" contains a single interaction. For instance, if the row \"$142, 152, 5$\" is present, this means that the user with ID $142$ has given the movie $152$ a positive rating of $5$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8dc9801",
      "metadata": {
        "id": "e8dc9801"
      },
      "source": [
        "The dataframe \"yy\" has two columns. In the first column we have the user IDs, whilst the second column contains the continuous label. A label of $0.01$ indicates a very low anomaly level, whilst a label of $0.99$ indicates a very high amount of noise/corruption."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df4ef3e7",
      "metadata": {
        "id": "df4ef3e7"
      },
      "source": [
        "### 3. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "876ae8f7",
      "metadata": {
        "id": "876ae8f7"
      },
      "source": [
        "Your task is to be able to regress the noise level $p$ for each new user, and predict the anomaly type $X$. The first (regression) task will be easier due to the larger amount of supervision, and will form the main basis of the evaluation. The second task will be more importance to showcase each team's creativity and differentiate between top performers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e059a16",
      "metadata": {
        "id": "5e059a16"
      },
      "source": [
        "THE **EVALUATION METRICs** are:  \n",
        "\n",
        "1. The Mean Absolute Error (MAE) for the regression task.\n",
        "2. The accuracy for the classiciation task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f42d14f",
      "metadata": {
        "id": "5f42d14f"
      },
      "source": [
        "Every few weeks, we will evaluate the performance of each team (on a *test set with unseen labels* that I will provide) in terms of both metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb35f65c",
      "metadata": {
        "id": "fb35f65c"
      },
      "source": [
        "The difficulty implied by **the generation procedure of the anomalies MAY CHANGE as the project evolves: depending on how well the teams are doing, I may generate easier or harder anomaly classes, which would change the number of labels in the classification task**. However, the regression task will still be the same (with a different distribution)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0065547d",
      "metadata": {
        "id": "0065547d"
      },
      "source": [
        "### 4. Deliverables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ac837a",
      "metadata": {
        "id": "22ac837a"
      },
      "source": [
        "Together with this file, you are provided with a first batch of examples \"`first_batch_regression_labelled.npz`\" which are labelled in terms of noise level. You are also provided with the test samples to rank by the next round (without labels) in the file \"`second_batch_regression_unlabelled.npz`\".\n",
        "\n",
        "The **first round** will take place after recess (week 9): you must hand in your scores for the second batch before the **Wednesday at NOON (15th of October)**. We will then look at the results together on the Friday.  \n",
        "\n",
        "We will check everyone's performance in this way every week (once on  week 10, once on week 11 and once on week 12).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "527121a4",
      "metadata": {
        "id": "527121a4"
      },
      "source": [
        "To summarise, the project deliverables are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3cab22a",
      "metadata": {
        "id": "f3cab22a"
      },
      "source": [
        "- Before every checkpoint's deadline, you need to submit **a `.csv` file** containing a dataframe of size $\\text{number of test batch users} \\times 3$.\n",
        "    - The first column should be the user IDs of the test batch.\n",
        "    - The second column should contain the estimated noise level $p$ for each sample.\n",
        "    - The final column should contain the estimated class (it should be a natural number in \\{0,1,2\\}).\n",
        "- The order of rows should correspond to the user IDs. For example, if the test batch contains users 1100-2200, scores for user 1100 should be the first row (row 0), scores for user 1101 should be the second row (row 1), and so on.\n",
        "- On Week 12-13 (schedule to be decided), you need to present your work in class. The presentation duration is **10 minutes** with 5 minutes of QA.\n",
        "- On Week 12, you need to submit your **Jupyter Notebook** (with comments in Markdown) and the **slides** for your presentation.\n",
        "- On week 13 you need to submit your **final report**. The final report should be 2-3 pages long (consisting of problem statement, literature review, and motivation of algorithm design) with unlimited references/appendix."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f3015c",
      "metadata": {
        "id": "31f3015c"
      },
      "source": [
        "Whilst performance (expressed in terms of MAE and accuracy) at **each of the check points** (weeks 9 to 12 inclusive) is an **important component** of your **final grade**, the **final report** and the detail of the various methods you will have tried will **also** be very **important**. Ideally, to get perfect marks (A+), you should try at least **two supervised methods** and **two unsupervised methods**, as well as be ranked the **best team** in terms of performance.\n",
        "\n",
        "\n",
        "In addition, I will be especially interested in your **reasoning**. Especially high marks will be awarded to any team that is able to **qualitatively describe** the difference between the two anomaly types. You are also encouraged to compute statistics related to each class and describe what is different about them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b5b0da-1a70-409b-8117-4e831ff7da13",
      "metadata": {
        "id": "54b5b0da-1a70-409b-8117-4e831ff7da13"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "oj_msJC0bB4w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj_msJC0bB4w",
        "outputId": "465f983e-3e71-4421-cd78-fe6c3ead4818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed scikit-learn-1.7.2\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras) (1.7.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# install scikeras\n",
        "!pip install scikeras\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a49b0983",
      "metadata": {
        "id": "a49b0983"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import skew\n",
        "\n",
        "data  = np.load(\"first_batch_regression_labelled.npz\")\n",
        "X     = data[\"X\"]\n",
        "y     = data[\"yy\"]\n",
        "y_cat = data[\"yy_cat\"]\n",
        "\n",
        "# Load dataframes\n",
        "X     = pd.DataFrame(X, columns=[\"user\", \"item\", \"rating\"])\n",
        "y     = pd.DataFrame(y, columns=[\"user\", \"label\"])\n",
        "y_cat = pd.DataFrame(y_cat, columns=[\"user\", \"label\", \"anomtype\"])\n",
        "\n",
        "# Parse to correct types\n",
        "y     = y.astype({\"user\": int, \"label\": float})\n",
        "y_cat = y_cat.astype({\"user\": int, \"label\": float, \"anomtype\": int})\n",
        "\n",
        "XX    = np.load(\"second_batch_regression_unlabelled.npz\")['X']\n",
        "XX    = pd.DataFrame(XX, columns=[\"user\", \"item\", \"rating\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ja-Ddf_grSBV",
      "metadata": {
        "id": "Ja-Ddf_grSBV"
      },
      "source": [
        "**Understanding the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e14ccea9-e508-4106-90eb-2ff8ac8dcd84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e14ccea9-e508-4106-90eb-2ff8ac8dcd84",
        "outputId": "82c9576a-a119-4aa1-8622-2a75032f0ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique users in first batch: 900\n",
            "Number of unique users in second batch: 900\n",
            "Number of unique movies in first batch: 1000\n",
            "Number of unique movies in second batch: 1000\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Count unique users\n",
        "unique_users_1 = X[\"user\"].nunique()\n",
        "unique_users_2 = XX[\"user\"].nunique()\n",
        "\n",
        "print(f\"Number of unique users in first batch: {unique_users_1}\")\n",
        "print(f\"Number of unique users in second batch: {unique_users_2}\")\n",
        "\n",
        "\n",
        "# Count unique movies\n",
        "unique_movies_1 = X[\"item\"].nunique()\n",
        "unique_movies_2 = XX[\"item\"].nunique()\n",
        "\n",
        "print(f\"Number of unique movies in first batch: {unique_movies_1}\")\n",
        "print(f\"Number of unique movies in second batch: {unique_movies_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Oek-n4ba_mex",
      "metadata": {
        "id": "Oek-n4ba_mex"
      },
      "source": [
        "900 users in both first and second batch\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1000 movies in both first and second batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sSIE3zKC_lbS",
      "metadata": {
        "id": "sSIE3zKC_lbS"
      },
      "source": [
        "Number of samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "165fe309",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "165fe309",
        "outputId": "27b55243-b085-4f12-c29c-58212ba102a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "288205\n",
            "282446\n",
            "900\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "print(len(X))\n",
        "print(len(XX))\n",
        "print(len(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EbytgxzHLxvr",
      "metadata": {
        "id": "EbytgxzHLxvr"
      },
      "source": [
        "Viewing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "vXur3D8hrZtn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXur3D8hrZtn",
        "outputId": "bc3e893d-b3f0-4672-eda4-6b6859db1733"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     user  item  rating\n",
            "0       0    94       2\n",
            "1       0    90       1\n",
            "2       0    97       2\n",
            "3       0   100       4\n",
            "4       0   101       2\n",
            "..    ...   ...     ...\n",
            "995     3   282       5\n",
            "996     3    70       5\n",
            "997     3   466       4\n",
            "998     3   464       5\n",
            "999     3   461       5\n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "print(X.head(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d9f83282",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9f83282",
        "outputId": "34e29732-06a7-408b-e9e8-21f2269b89b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     user  item  rating\n",
            "0     900     0       2\n",
            "1     900   388       2\n",
            "2     900   389       3\n",
            "3     900   390       0\n",
            "4     900   401       5\n",
            "..    ...   ...     ...\n",
            "995   903   724       5\n",
            "996   903   266       3\n",
            "997   903   219       3\n",
            "998   903   299       5\n",
            "999   903   773       4\n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "print(XX.head(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "FHP71VtxOFwX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHP71VtxOFwX",
        "outputId": "8070d56a-0a5c-4722-b232-888c3073abc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   user     label\n",
            "0     0  0.962817\n",
            "1     1  0.031248\n",
            "2     2  0.068668\n",
            "3     3  0.349012\n",
            "4     4  0.917704\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "print(y.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "tHHLXuEaZp6V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHHLXuEaZp6V",
        "outputId": "25dbf0b0-04ce-4696-90c8-7ca6868dfb24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    user     label  anomtype\n",
            "0    561  0.383316         1\n",
            "1    202  0.925028         2\n",
            "2    205  0.380860         2\n",
            "3    424  0.255181         1\n",
            "4    284  0.055162         2\n",
            "5    667  0.558745         0\n",
            "6    730  0.311928         1\n",
            "7    469  0.233492         2\n",
            "8    199  0.165112         1\n",
            "9    699  0.261752         2\n",
            "10   231  0.951103         0\n",
            "11    26  0.558222         0\n",
            "12   786  0.549116         0\n",
            "13   849  0.301816         1\n",
            "14   459  0.739300         0\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "print(y_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K0ule6uAL9zn",
      "metadata": {
        "id": "K0ule6uAL9zn"
      },
      "source": [
        "No. of ratings per user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "XE2nvRnNsFPK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "XE2nvRnNsFPK",
        "outputId": "cc31ebfe-7669-463b-e616-bb58641a6862"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>user</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "user\n",
              "125    918\n",
              "290    764\n",
              "764    761\n",
              "452    752\n",
              "307    752\n",
              "      ... \n",
              "192    200\n",
              "153    200\n",
              "477    200\n",
              "848    200\n",
              "390    200\n",
              "Name: count, Length: 900, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title\n",
        "X[\"user\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4z2PjyeTL_5z",
      "metadata": {
        "id": "4z2PjyeTL_5z"
      },
      "source": [
        "No. of ratings per movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "SkXrR6N3Bm_x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "SkXrR6N3Bm_x",
        "outputId": "155ba559-a9a8-4925-daa9-f2670657e37e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842</th>\n",
              "      <td>124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "item\n",
              "404    610\n",
              "425    590\n",
              "240    586\n",
              "51     581\n",
              "94     576\n",
              "      ... \n",
              "990    129\n",
              "53     128\n",
              "842    124\n",
              "623    122\n",
              "458    120\n",
              "Name: count, Length: 1000, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title\n",
        "X[\"item\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H6FO8C1QMZSu",
      "metadata": {
        "id": "H6FO8C1QMZSu"
      },
      "source": [
        "Check if there are duplicates for any user, movie pair rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "uHvi0IVUsah5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHvi0IVUsah5",
        "outputId": "121ec2bf-5d58-4ab7-ee57-2afb3b9b55d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All repeated (user,item) rows for user:\n",
            "        user  item  rating\n",
            "262139   816    24       4\n",
            "262223   816    24       4\n",
            "262319   816    24       5\n",
            "262233   816    34       4\n",
            "262250   816    34       4\n",
            "...      ...   ...     ...\n",
            "262394   816   996       5\n",
            "262398   816   996       5\n",
            "262439   816   999       4\n",
            "262242   816   999       5\n",
            "262464   816   999       5\n",
            "\n",
            "[224 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "user_df = X[X[\"user\"] == 816].copy()\n",
        "\n",
        "# Items user rated more than once (regardless of rating value)\n",
        "multi = (user_df.groupby(\"item\")\n",
        "         .agg(n_ratings=(\"rating\", \"size\"),\n",
        "              unique_ratings=(\"rating\", \"nunique\"),\n",
        "              ratings=(\"rating\", lambda s: list(s)))\n",
        "         .reset_index())\n",
        "\n",
        "# 1) All duplicated (user,item)\n",
        "dupe_items = multi.loc[multi[\"n_ratings\"] > 1, \"item\"]\n",
        "dupe_rows = user_df[user_df[\"item\"].isin(dupe_items)].sort_values([\"item\", \"rating\"])\n",
        "print(\"All repeated (user,item) rows for user:\")\n",
        "print(dupe_rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0LX_EMeZtjv_",
      "metadata": {
        "id": "0LX_EMeZtjv_"
      },
      "source": [
        "Confirmed that some users have duplicate ratings for the same item. But not all users have duplicate ratings (eg user 1 doesnt, but user 3 has)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "mmjFuc3SP6QO",
      "metadata": {
        "id": "mmjFuc3SP6QO"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split users (each row is unique)\n",
        "y_train, y_temp = train_test_split(y, test_size=0.2, random_state=88)\n",
        "y_val, y_test = train_test_split(y_temp, test_size=0.5, random_state=88)\n",
        "\n",
        "# Then subset X by these users\n",
        "train_subset = X[X['user'].isin(y_train['user'])]\n",
        "val_subset   = X[X['user'].isin(y_val['user'])]\n",
        "test_subset  = X[X['user'].isin(y_test['user'])]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XoDnOBVLezEE",
      "metadata": {
        "id": "XoDnOBVLezEE"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AQ1_AYXjUX77",
      "metadata": {
        "id": "AQ1_AYXjUX77"
      },
      "source": [
        "creating features for data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "k0U3XIrVtyCE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0U3XIrVtyCE",
        "outputId": "ea840b44-51f9-4bc9-e9bb-f0653a0d0b30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['user', 'user_mean_mean', 'user_std_mean', 'user_skew_mean',\n",
              "       'user_min_mean', 'user_max_mean', 'user_count_mean',\n",
              "       'global_zscore_mean', 'global_zscore_std', 'user_zscore_mean',\n",
              "       'user_zscore_std', 'item_mean_mean', 'item_mean_std', 'item_std_mean',\n",
              "       'item_std_std', 'item_zscore_mean', 'item_zscore_std', 'item_skew_mean',\n",
              "       'item_skew_std', 'item_min_mean', 'item_min_std', 'item_max_mean',\n",
              "       'item_max_std', 'item_count_mean', 'item_count_std', 'label'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def create_comprehensive_features(df):\n",
        "    \"\"\"\n",
        "    Create comprehensive statistical features for ratings.\n",
        "\n",
        "    Features created:\n",
        "    - Global z-score for all ratings\n",
        "    - Per user features: mean, std, z-score, skew, min, max, count\n",
        "    - Per item features: mean, std, z-score, skew, min, max, count\n",
        "\n",
        "    Parameters:\n",
        "    df : DataFrame with columns ['user', 'item', 'rating']\n",
        "\n",
        "    Returns:\n",
        "    DataFrame with original columns plus all engineered features\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # ============ Global Z-Score ============\n",
        "    df['global_zscore'] = stats.zscore(df['rating'], nan_policy='omit')\n",
        "\n",
        "    # ============ Per User Features ============\n",
        "    user_stats = df.groupby('user')['rating'].agg([\n",
        "        ('user_mean', 'mean'),\n",
        "        ('user_std', 'std'),\n",
        "        ('user_skew', lambda x: stats.skew(x)),\n",
        "        ('user_min', 'min'),\n",
        "        ('user_max', 'max'),\n",
        "        ('user_count', 'count')\n",
        "    ]).reset_index()\n",
        "\n",
        "    # Calculate user z-score\n",
        "    user_zscore = df.groupby('user')['rating'].apply(\n",
        "        lambda x: pd.Series(stats.zscore(x, nan_policy='omit'), index=x.index)\n",
        "    ).reset_index(level=0, drop=True).to_frame(name='user_zscore')\n",
        "\n",
        "    df = df.merge(user_stats, on='user', how='left')\n",
        "    df = df.merge(user_zscore, left_index=True, right_index=True, how='left')\n",
        "\n",
        "    # ============ Per Item Features ============\n",
        "    item_stats = df.groupby('item')['rating'].agg([\n",
        "        ('item_mean', 'mean'),\n",
        "        ('item_std', 'std'),\n",
        "        ('item_skew', lambda x: stats.skew(x)),\n",
        "        ('item_min', 'min'),\n",
        "        ('item_max', 'max'),\n",
        "        ('item_count', 'count')\n",
        "    ]).reset_index()\n",
        "\n",
        "    # Calculate item z-score\n",
        "    item_zscore = df.groupby('item')['rating'].apply(\n",
        "        lambda x: pd.Series(stats.zscore(x, nan_policy='omit'), index=x.index)\n",
        "    ).reset_index(level=0, drop=True).to_frame(name='item_zscore')\n",
        "\n",
        "    df = df.merge(item_stats, on='item', how='left')\n",
        "    df = df.merge(item_zscore, left_index=True, right_index=True, how='left')\n",
        "\n",
        "    # ============ Smart NaN Filling ============\n",
        "    # Fill std with 0 (no variability when only 1 rating)\n",
        "    df['user_std'] = df['user_std'].fillna(0)\n",
        "    df['item_std'] = df['item_std'].fillna(0)\n",
        "\n",
        "    # Fill skewness with global skewness\n",
        "    global_skew = stats.skew(df['rating'])\n",
        "    df['user_skew'] = df['user_skew'].fillna(global_skew)\n",
        "    df['item_skew'] = df['item_skew'].fillna(global_skew)\n",
        "\n",
        "    # Fill z-scores with global z-score\n",
        "    df['user_zscore'] = df['user_zscore'].fillna(df['global_zscore'])\n",
        "    df['item_zscore'] = df['item_zscore'].fillna(df['global_zscore'])\n",
        "\n",
        "    # Fill any remaining NaN with 0\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "X_features = create_comprehensive_features(X)\n",
        "\n",
        "# Features that need both mean AND std aggregation\n",
        "features_with_std = [\n",
        "    'global_zscore', 'user_zscore', 'item_mean', 'item_std', 'item_zscore',\n",
        "    'item_skew', 'item_min', 'item_max', 'item_count'\n",
        "]\n",
        "\n",
        "# All other numeric features (will be aggregated by mean only)\n",
        "all_features = X_features.select_dtypes(include=['number']).columns.tolist()\n",
        "features_mean_only = [f for f in all_features if f not in features_with_std]\n",
        "\n",
        "# Build aggregation dictionary\n",
        "agg_dict = {}\n",
        "\n",
        "# Features aggregated by mean only\n",
        "for feat in features_mean_only:\n",
        "    agg_dict[feat] = 'mean'\n",
        "\n",
        "# Features aggregated by both mean and std\n",
        "for feat in features_with_std:\n",
        "    agg_dict[feat] = ['mean', 'std']\n",
        "\n",
        "# Group by user\n",
        "X_grouped = X_features.groupby('user').agg(agg_dict).reset_index()\n",
        "\n",
        "# Flatten column names for easier access\n",
        "X_grouped.columns = ['_'.join(col).strip('_') if col[1] else col[0]\n",
        "                     for col in X_grouped.columns.values]\n",
        "\n",
        "combined = X_grouped.merge(y, on='user', how='left').drop(['user_mean', 'item_mean', 'rating_mean'], axis=1)\n",
        "combined.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JkL64AZYlH1x",
      "metadata": {
        "id": "JkL64AZYlH1x"
      },
      "source": [
        "Split into train, test 80:20 ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "GFyrw7IYlOaE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFyrw7IYlOaE",
        "outputId": "d7555366-7c02-4129-af49-afecffaaee82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Train-Test Split (80:20)\n",
            "======================================================================\n",
            "X_train shape: (720, 24)\n",
            "X_test shape: (180, 24)\n",
            "y_train shape: (720,)\n",
            "y_test shape: (180,)\n",
            "\n",
            "Train set: 80.0%\n",
            "Test set: 20.0%\n"
          ]
        }
      ],
      "source": [
        "X_combined = combined.drop(['user', 'label'], axis=1)\n",
        "y_combined = combined['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_combined, y_combined,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"Train-Test Split (80:20)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"\\nTrain set: {len(X_train) / len(X_combined) * 100:.1f}%\")\n",
        "print(f\"Test set: {len(X_test) / len(X_combined) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Aw0t4LOh8qPN",
      "metadata": {
        "id": "Aw0t4LOh8qPN"
      },
      "source": [
        "PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "CLr8HiPW8orF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CLr8HiPW8orF",
        "outputId": "38eaa1d6-9e64-47d0-8fbe-5b792ad1f61f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eigenvectors for each PC: \n",
            "[[-2.77474218e-01  1.98351772e-01  1.89943570e-01 -9.29940309e-02\n",
            "  -6.96323060e-02 -2.30353860e-02 -2.77474218e-01  1.98351772e-01\n",
            "   8.95515356e-03  2.37049601e-02 -2.88947916e-01 -1.19229675e-01\n",
            "   2.90864630e-01 -2.80945436e-01 -2.16512322e-01  2.17206003e-01\n",
            "   2.89487421e-01 -1.83722522e-01 -2.59583892e-01 -2.55883132e-01\n",
            "  -0.00000000e+00 -0.00000000e+00  1.93383390e-01  2.71057466e-01]\n",
            " [-1.41914646e-01  3.03465455e-01 -1.82034113e-02 -3.13028504e-01\n",
            "   5.16545204e-02  3.42954536e-01 -1.41914646e-01  3.03465455e-01\n",
            "  -5.71351313e-03 -3.33263196e-01  5.67772795e-02  2.83539393e-01\n",
            "  -1.09083527e-01  1.36542517e-01 -2.17729483e-01  3.03820739e-01\n",
            "  -6.00412879e-02  2.82362326e-01  8.59721802e-02  1.00890492e-01\n",
            "  -0.00000000e+00 -0.00000000e+00 -2.86973044e-01 -9.33012499e-02]\n",
            " [ 8.82834997e-02  3.35643109e-01 -2.80890744e-01 -2.19149326e-01\n",
            "   1.43395787e-01 -4.92077358e-01  8.82834997e-02  3.35643109e-01\n",
            "   5.40301987e-03  5.00657497e-01  8.29606758e-02  8.28209110e-02\n",
            "  -2.21976824e-02  1.13891332e-01  7.61403785e-02  2.09788947e-01\n",
            "  -7.70557128e-02  6.76638001e-02  4.04970326e-02  3.91834669e-02\n",
            "   0.00000000e+00  0.00000000e+00  1.67452769e-01  6.12799740e-03]\n",
            " [ 1.56385444e-01 -1.08302174e-01 -4.49541856e-02 -7.34981412e-02\n",
            "   2.59280701e-01  1.15818766e-02  1.56385444e-01 -1.08302174e-01\n",
            "  -6.72776245e-02 -2.05765271e-02 -2.34743229e-01  4.97619323e-01\n",
            "   1.50532683e-01 -2.14489170e-02  3.12340347e-01 -9.23727954e-02\n",
            "   2.26644155e-01  3.94570328e-01 -2.85956327e-01 -2.56008346e-01\n",
            "  -0.00000000e+00 -0.00000000e+00 -8.28505426e-02  2.33939622e-01]\n",
            " [-2.26683642e-01 -1.26550860e-01  4.25479448e-01  2.63887323e-01\n",
            "  -2.16645737e-01 -3.20260330e-01 -2.26683642e-01 -1.26550860e-01\n",
            "  -7.90821737e-03  3.27143264e-01  1.22076410e-02  3.17067465e-01\n",
            "  -3.39758435e-02  1.51345383e-01 -3.09650502e-01 -1.47146858e-02\n",
            "  -1.48143227e-02  3.02355800e-01  4.92132598e-02  6.80239044e-02\n",
            "   0.00000000e+00  0.00000000e+00 -1.95340556e-01 -1.60018199e-02]\n",
            " [ 1.55039056e-02  1.97067924e-02  8.10721348e-02  7.45185847e-02\n",
            "   1.61948451e-01  3.04353997e-02  1.55039056e-02  1.97067924e-02\n",
            "   9.77267625e-01  5.31275916e-03 -1.09842869e-02  3.20339462e-02\n",
            "   2.14689444e-02  2.31122774e-03  2.42965610e-02 -2.88483131e-04\n",
            "   1.09186092e-02  1.91360870e-02  2.03109668e-02  2.54594996e-02\n",
            "  -0.00000000e+00 -0.00000000e+00  7.23355121e-03  2.41047298e-02]\n",
            " [-2.95203944e-02  5.25411479e-02  3.40481989e-01  2.53909430e-01\n",
            "   8.51775817e-01  4.14193344e-04 -2.95203944e-02  5.25411479e-02\n",
            "  -1.86805502e-01  9.38863701e-03  1.51228653e-02 -1.14470078e-01\n",
            "  -1.74590139e-02 -2.58881952e-02 -4.91787549e-02  6.08307027e-02\n",
            "  -8.58828362e-03 -8.66097061e-02  1.07815213e-01  1.03248526e-01\n",
            "  -0.00000000e+00 -0.00000000e+00 -3.42112275e-02 -1.93773732e-02]\n",
            " [ 1.82249260e-01  2.76010499e-01  6.69807792e-02  6.58133630e-01\n",
            "  -2.76615223e-01  9.22707008e-02  1.82249260e-01  2.76010499e-01\n",
            "  -5.32538784e-02 -8.98233952e-02 -7.75866487e-02  6.66236531e-02\n",
            "   6.09229089e-02 -5.91447784e-02  2.79839580e-01  2.65130267e-01\n",
            "   8.82837149e-02  4.75399845e-02  1.35626664e-01  1.67474122e-01\n",
            "  -0.00000000e+00 -0.00000000e+00 -8.62443752e-03  1.57491173e-01]\n",
            " [-3.87948571e-02 -1.50805724e-01 -1.30944497e-01 -2.54088280e-01\n",
            "   1.43077066e-02 -9.21237741e-02 -3.87948571e-02 -1.50805724e-01\n",
            "  -5.50045098e-04  4.86142454e-02 -2.21071078e-01 -2.25569688e-02\n",
            "   1.92317024e-01 -2.06508449e-01  5.08390417e-02  4.46978839e-02\n",
            "   2.45831171e-01 -3.42592339e-02  4.71264989e-01  5.27432320e-01\n",
            "  -0.00000000e+00 -0.00000000e+00 -2.49560587e-01  3.17780913e-01]\n",
            " [-2.65989250e-02  1.05032550e-01  2.05662946e-01 -1.09046486e-01\n",
            "  -2.25212592e-02  1.02525127e-01 -2.65989250e-02  1.05032550e-01\n",
            "  -4.00579253e-02 -1.38898632e-01  5.46912793e-02  2.25586809e-01\n",
            "   2.66540610e-02  1.09922149e-01 -4.56869277e-02 -4.16550135e-01\n",
            "  -4.94468806e-02  1.11377533e-01  2.15282202e-01  2.28015053e-01\n",
            "   0.00000000e+00  0.00000000e+00  6.97971127e-01  2.18610969e-01]\n",
            " [ 2.31487060e-01  6.77251058e-02  7.05752619e-01 -4.26926099e-01\n",
            "  -1.67318760e-01 -5.30551351e-02  2.31487060e-01  6.77251058e-02\n",
            "  -4.46408328e-03  4.50126804e-02 -3.06152038e-02 -9.93073799e-02\n",
            "  -7.42928495e-02 -8.79722923e-02  3.12489121e-01  5.69649302e-02\n",
            "   2.72166156e-02 -5.37033802e-02  2.30335876e-02 -3.92434435e-03\n",
            "   0.00000000e+00  0.00000000e+00 -1.00673838e-01 -1.76597700e-01]\n",
            " [ 8.03251548e-02  3.14443588e-02  1.07984425e-01 -6.21389537e-02\n",
            "  -2.17273770e-02  8.03449783e-02  8.03251548e-02  3.14443588e-02\n",
            "  -1.06795636e-02  3.49434481e-02  2.48369387e-01 -2.24689611e-01\n",
            "   4.29997412e-01  5.41658195e-01 -3.10369527e-02 -5.38300602e-02\n",
            "  -1.94003998e-01 -6.70125013e-02 -1.07030850e-01 -4.14525077e-02\n",
            "   0.00000000e+00  0.00000000e+00 -2.47063417e-01  5.00665834e-01]\n",
            " [-6.43944545e-03 -1.53545109e-01 -5.42154080e-03 -1.48062139e-02\n",
            "   6.04102973e-03  4.27590793e-01 -6.43944545e-03 -1.53545109e-01\n",
            "  -1.83793762e-02  3.57650745e-01 -1.68743231e-01  5.93961575e-02\n",
            "   3.13536208e-01  3.74282931e-01  5.66620416e-02  3.26083964e-01\n",
            "   1.84142935e-01 -4.85986690e-02  9.27255644e-02  5.67904104e-02\n",
            "   3.38813179e-21  0.00000000e+00  2.65885063e-01 -3.77715934e-01]\n",
            " [ 1.27126902e-02  4.55351599e-02  4.43831944e-03 -1.30257074e-02\n",
            "  -1.09794900e-02  5.57681636e-01  1.27126902e-02  4.55351599e-02\n",
            "  -1.61709668e-02  5.91501335e-01  1.34087574e-01 -5.45695576e-03\n",
            "  -2.27267237e-01 -3.70334470e-01 -3.96596988e-02 -1.27781470e-01\n",
            "  -1.62642891e-01  7.24659252e-02 -5.74427928e-02  8.43176087e-03\n",
            "   6.77626358e-21  0.00000000e+00 -9.07837703e-02  2.57004027e-01]\n",
            " [ 2.29409912e-02 -2.48649213e-01  6.04849755e-02 -5.95559177e-02\n",
            "   6.35731176e-03 -2.93242219e-02  2.29409912e-02 -2.48649213e-01\n",
            "   1.06118179e-02 -8.72023079e-02  3.13793128e-02 -2.83466285e-02\n",
            "  -5.09002514e-01  1.45329143e-01  1.05706694e-02  5.66345440e-01\n",
            "  -4.53807804e-02 -8.16054652e-04 -3.53131160e-02 -2.67525373e-02\n",
            "   0.00000000e+00  0.00000000e+00  2.61229843e-01  4.29368147e-01]\n",
            " [ 9.40981736e-02 -1.47870996e-01  3.54844198e-02 -2.31164563e-02\n",
            "   9.19820496e-03 -6.52200677e-02  9.40981736e-02 -1.47870996e-01\n",
            "  -2.67979841e-03 -1.07219432e-01  4.03271073e-01  1.12985025e-01\n",
            "   4.82355116e-01 -4.54811538e-01 -1.04129780e-01  3.28372653e-01\n",
            "  -3.48009674e-01  1.75680027e-01 -2.11852982e-02  4.43456114e-02\n",
            "  -0.00000000e+00 -0.00000000e+00  1.63983362e-01 -7.78431850e-02]\n",
            " [-2.99526245e-02  5.38596832e-03 -2.04535891e-02  5.77109049e-03\n",
            "   3.28004713e-03 -4.25370422e-03 -2.99526245e-02  5.38596832e-03\n",
            "   4.80196871e-03 -1.05795434e-02 -1.14971663e-01 -6.29131288e-01\n",
            "   4.34184676e-03  8.86135365e-03  2.50348829e-02 -1.53762298e-02\n",
            "   7.15051710e-02  7.37840650e-01 -1.03592451e-01  1.13703937e-01\n",
            "   3.46944695e-18 -0.00000000e+00  1.05259081e-01 -5.73209433e-02]\n",
            " [-7.43215476e-04 -6.98273060e-04 -6.83985822e-03  6.42568459e-03\n",
            "  -2.80998550e-03  1.61158414e-02 -7.43215476e-04 -6.98273060e-04\n",
            "   1.08884942e-03  1.13111079e-02  4.37340362e-02 -8.33779367e-02\n",
            "   3.24960363e-02 -2.59580941e-02 -2.50578020e-03  3.07898548e-03\n",
            "   2.48530405e-02  1.43371122e-01  7.08504025e-01 -6.79658469e-01\n",
            "  -3.33066907e-16 -0.00000000e+00 -5.80919020e-03  6.15036232e-02]\n",
            " [ 1.70885034e-01  6.98734030e-03 -3.95481356e-03  1.81486224e-03\n",
            "  -2.50583123e-03  8.23465088e-03  1.70885034e-01  6.98734030e-03\n",
            "   1.50779365e-03  7.15879963e-03  5.61980100e-01 -1.20418825e-02\n",
            "  -6.13101711e-02 -9.43192704e-03 -2.70762527e-01 -2.09994145e-02\n",
            "   7.38189599e-01  1.75994410e-02 -4.43258965e-02  1.92339168e-02\n",
            "  -3.81639165e-17  0.00000000e+00  6.95294630e-03 -1.29562097e-02]\n",
            " [-4.27751847e-01 -8.00096143e-03  3.34374881e-03 -3.20249359e-03\n",
            "   2.65614422e-03  7.49548204e-05 -4.27751847e-01 -8.00096143e-03\n",
            "   5.79032162e-04  1.56677687e-03  4.35055809e-01  5.46309210e-03\n",
            "   7.83621752e-03  1.83430499e-03  6.57412880e-01  1.35368745e-02\n",
            "   1.07880432e-01  1.55100304e-03 -1.64264019e-02  1.28050485e-02\n",
            "  -1.87350135e-16 -0.00000000e+00 -6.34157213e-03 -6.29885532e-03]\n",
            " [-1.61058284e-02 -7.06923336e-01 -1.60327968e-15  1.19969097e-15\n",
            "  -1.16494025e-15  3.85418110e-15  1.61058284e-02  7.06923336e-01\n",
            "   3.07668019e-16  4.25174247e-15  1.90766541e-13 -1.51933945e-14\n",
            "  -2.49517908e-14 -2.26503040e-15 -1.13317561e-13 -1.52775662e-14\n",
            "   2.72479962e-13  1.66498444e-14 -2.70976688e-14  1.81811308e-14\n",
            "  -4.81660860e-07 -1.11022302e-16  1.66955843e-15 -8.60729726e-15]\n",
            " [-3.17528545e-18  3.72334462e-17 -9.40380200e-31  8.66092130e-31\n",
            "  -1.92517720e-31  3.43010485e-30  3.17528545e-18 -3.72334462e-17\n",
            "   3.24193513e-31  2.58578843e-30  7.72451476e-29 -1.50470057e-29\n",
            "   7.76213923e-30 -5.32488655e-30  9.48658270e-29  2.93951285e-30\n",
            "   2.51230584e-29  2.86073979e-29  1.47694427e-28 -1.42307774e-28\n",
            "  -1.43704382e-14  1.00000000e+00 -2.27035080e-30  1.22822645e-29]\n",
            " [-6.81009901e-06 -1.85519613e-07  1.03154608e-18 -1.09756031e-18\n",
            "   7.34208587e-19 -9.77413113e-19  6.81009901e-06  1.85519613e-07\n",
            "   9.28286467e-20 -3.36202069e-19  1.04573000e-16  5.93919827e-18\n",
            "  -2.45531837e-18  2.11409570e-18  1.32082554e-16  1.93694730e-18\n",
            "   4.19585387e-17 -8.61248124e-18 -5.91811975e-17  5.55541239e-17\n",
            "   1.00000000e+00  1.42264130e-14 -5.18780044e-19 -6.62703703e-18]\n",
            " [ 7.06923335e-01 -1.61058284e-02 -9.85692237e-15  1.04536342e-14\n",
            "  -1.04990208e-14  7.66424028e-16 -7.06923335e-01  1.61058284e-02\n",
            "  -1.82177067e-15 -4.52126406e-15 -1.50450570e-12 -1.41290765e-14\n",
            "  -3.34177087e-14 -9.51129367e-15 -2.38696980e-12 -5.00275770e-14\n",
            "  -3.19525691e-13 -1.14129982e-14  6.94871908e-14 -5.95056343e-14\n",
            "   9.62245992e-06  3.46944695e-18  2.06764478e-14  2.59087870e-14]]\n",
            "\n",
            "Principal Component Scores: \n",
            "[[ 8.56417526e-01 -1.72863230e+00  1.83776036e-01 ...  3.37268433e-31\n",
            "  -3.16239273e-18  4.91580488e-14]\n",
            " [ 7.89971589e-01 -4.51577437e+00 -1.30227727e+00 ... -5.26025736e-30\n",
            "  -5.41229824e-18  7.83228572e-14]\n",
            " [-2.99487873e-01  1.78262323e+00  1.96095606e+00 ... -8.69298343e-30\n",
            "   3.20028384e-18  9.56936058e-15]\n",
            " ...\n",
            " [ 4.63286448e+00 -1.09595578e+00  7.25763849e-01 ...  2.17612316e-29\n",
            "  -2.46337224e-18 -4.16030299e-14]\n",
            " [-5.39589526e-02  1.80229057e+00  5.44623443e-01 ... -1.67289869e-29\n",
            "   6.76842769e-18  9.00181264e-15]\n",
            " [-7.42386590e+00 -1.55366238e+00  1.10216316e+00 ...  6.30217115e-29\n",
            "  -2.68593891e-17  4.96484994e-14]]\n",
            "\n",
            "Eigenvalues: \n",
            "[1.00899822e+01 3.08281279e+00 1.81202836e+00 1.63458230e+00\n",
            " 1.43848366e+00 1.00433827e+00 8.93116914e-01 6.51994804e-01\n",
            " 5.01750291e-01 4.16811284e-01 1.78289398e-01 1.17781628e-01\n",
            " 6.96969210e-02 5.66576925e-02 3.60049124e-02 3.26355627e-02\n",
            " 7.01871123e-03 5.33616090e-03 1.03359773e-03 2.42593859e-04\n",
            " 1.21851288e-15 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            "\n",
            "Proportion of Total Variation: \n",
            "[4.57998561e-01 1.39933232e-01 8.22505296e-02 7.41960022e-02\n",
            " 6.52948077e-02 4.55883343e-02 4.05398397e-02 2.95949662e-02\n",
            " 2.27751553e-02 1.89196536e-02 8.09280792e-03 5.34627465e-03\n",
            " 3.16364181e-03 2.57177278e-03 1.63431389e-03 1.48137434e-03\n",
            " 3.18589228e-04 2.42215889e-04 4.69164625e-05 1.10116783e-05\n",
            " 5.53100226e-17 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            "\n",
            "Cumulative Variation: \n",
            "[ 45.79985606  59.79317928  68.01823224  75.43783245  81.96731323\n",
            "  86.52614666  90.58013063  93.53962725  95.81714277  97.70910814\n",
            "  98.51838893  99.05301639  99.36938057  99.62655785  99.78998924\n",
            "  99.93812667  99.9699856   99.99420719  99.99889883 100.\n",
            " 100.         100.         100.         100.        ]\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# PCA on data to see importance of features\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA()\n",
        "principal_components = pca.fit_transform(X_scaled)\n",
        "\n",
        "# PCA results\n",
        "# Loadings / Eigenvectors for each PC (i.e. unit vector for each PC's direction)\n",
        "components = pca.components_\n",
        "print(f\"Eigenvectors for each PC: \\n{components}\\n\")\n",
        "\n",
        "# PC Scores (i.e. distance from origin to each point's projection on each PC)\n",
        "p_components = principal_components\n",
        "print(f\"Principal Component Scores: \\n{p_components}\\n\")\n",
        "\n",
        "# Eigenvalues for each PC (i.e. variation captured by each PC)\n",
        "explained_variance = pca.explained_variance_\n",
        "print(f\"Eigenvalues: \\n{explained_variance}\\n\")\n",
        "\n",
        "# Proportion of total variation explained by each PC\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(f\"Proportion of Total Variation: \\n{explained_variance_ratio}\\n\")\n",
        "\n",
        "# Cumulative variation\n",
        "cumulative_variance = np.cumsum(explained_variance_ratio) * 100\n",
        "print(f\"Cumulative Variation: \\n{cumulative_variance}\\n\")\n",
        "\n",
        "# Create a DataFrame for Scree Plot\n",
        "explained_variance_df = pd.DataFrame({\n",
        "    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
        "    'Explained Variance Ratio': explained_variance_ratio\n",
        "})\n",
        "\n",
        "# Plot the Scree Plot using Seaborn\n",
        "explained_variance_df.plot(\n",
        "    x='Principal Component',\n",
        "   y='Explained Variance Ratio',\n",
        "    kind='bar',\n",
        "    legend=False,\n",
        "    figsize=(6,4))\n",
        "plt.title('Scree Plot (Percentage of Variation Explained)')\n",
        "plt.ylabel('Percentage of Variation Explained (%)')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "180zpoH88zxk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "180zpoH88zxk",
        "outputId": "99fee061-2db8-45fd-fd38-2e39af9798f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variable Relative Importance (Sorted Descending, %):\n",
            "item_zscore_mean      5.160813e+00\n",
            "user_std_mean         5.158539e+00\n",
            "global_zscore_std     5.158539e+00\n",
            "global_zscore_mean    5.054875e+00\n",
            "user_mean_mean        5.054875e+00\n",
            "item_zscore_std       5.001402e+00\n",
            "item_skew_std         4.968885e+00\n",
            "item_count_mean       4.905517e+00\n",
            "item_min_std          4.884135e+00\n",
            "item_std_std          4.873794e+00\n",
            "item_min_mean         4.855853e+00\n",
            "item_count_std        4.764812e+00\n",
            "item_std_mean         4.692733e+00\n",
            "item_skew_mean        4.690398e+00\n",
            "item_mean_mean        4.689553e+00\n",
            "user_skew_mean        4.652982e+00\n",
            "item_mean_std         4.590344e+00\n",
            "user_min_mean         4.535215e+00\n",
            "user_max_mean         3.601666e+00\n",
            "user_count_mean       3.497880e+00\n",
            "user_zscore_std       3.480276e+00\n",
            "user_zscore_mean      1.726913e+00\n",
            "item_max_mean         2.257748e-18\n",
            "item_max_std          1.617971e-31\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# finding relative importance of each variable using PCA\n",
        "def create_importance_dataframe(pca, df): # create dataframe of eigenvectors for all the PCs\n",
        "\n",
        "    # Change pcs components ndarray to a dataframe\n",
        "    importance_df  = pd.DataFrame(pca.components_)\n",
        "\n",
        "    # Assign columns\n",
        "    importance_df.columns  = df.columns\n",
        "\n",
        "    # Change to absolute values\n",
        "    importance_df = importance_df.apply(np.abs)\n",
        "\n",
        "    # Transpose\n",
        "    importance_df = importance_df.transpose()\n",
        "\n",
        "    # Change column names\n",
        "    num_pcs = importance_df.shape[1]\n",
        "    new_columns = [f'PC{i}' for i in range(1, num_pcs + 1)]\n",
        "    importance_df.columns = new_columns\n",
        "\n",
        "    # Return importance df\n",
        "    return importance_df\n",
        "\n",
        "# Call function to create importance df\n",
        "# NOTE: Assuming 'pca' and 'X_train' are available.\n",
        "importance_df = create_importance_dataframe(pca, X_train)\n",
        "\n",
        "# modify dataframe to show importance of feature in entire dataset\n",
        "# NOTE: It is assumed 'explained_variance' holds the explained variance ratio (pca.explained_variance_ratio_).\n",
        "for i in range(len(explained_variance)):\n",
        "    importance_df[importance_df.columns[i]] = importance_df[importance_df.columns[i]] * explained_variance[i]\n",
        "\n",
        "# sum for each variable, normalize to 100%, and sort in descending order\n",
        "final_importance_series = importance_df.transpose().sum()\n",
        "final_importance_series = (final_importance_series / final_importance_series.sum()) * 100\n",
        "final_importance_series = final_importance_series.sort_values(ascending=False)\n",
        "\n",
        "\n",
        "# The final_importance_series now contains the relative importance (%) of each feature\n",
        "print(\"Variable Relative Importance (Sorted Descending, %):\")\n",
        "print(final_importance_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LLkRhcPwx3z4",
      "metadata": {
        "id": "LLkRhcPwx3z4"
      },
      "source": [
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "IGBcDgIYx38k",
      "metadata": {
        "id": "IGBcDgIYx38k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_validate, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "def linear_regression_cv(X_train, y_train, X_test, y_test, cv=5):\n",
        "    \"\"\"\n",
        "    Perform linear regression with cross-validation and scaling.\n",
        "\n",
        "    Parameters:\n",
        "    X_train : array-like, training features\n",
        "    y_train : array-like, training target\n",
        "    X_test : array-like, testing features\n",
        "    y_test : array-like, testing target\n",
        "    cv : int, number of cross-validation folds (default=5)\n",
        "\n",
        "    Returns:\n",
        "    dict with model, CV results, train MAE, and test MAE\n",
        "    \"\"\"\n",
        "\n",
        "    # Create pipeline: Scaling -> Linear Regression\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('linear_reg', LinearRegression())\n",
        "    ])\n",
        "\n",
        "    print(\"Performing Linear Regression with Cross-Validation and Scaling\")\n",
        "    print(f\"Cross-validation folds: {cv}\\n\")\n",
        "\n",
        "    # Perform cross-validation\n",
        "    print(\"Running cross-validation on training data...\")\n",
        "    cv_results = cross_validate(\n",
        "        pipeline,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cv=cv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        return_train_score=True\n",
        "    )\n",
        "\n",
        "    # Extract CV scores\n",
        "    cv_mae_scores = -cv_results['test_score']  # Convert to positive MAE\n",
        "    train_mae_scores = -cv_results['train_score']\n",
        "\n",
        "    # Calculate statistics\n",
        "    mean_cv_mae = cv_mae_scores.mean()\n",
        "    std_cv_mae = cv_mae_scores.std()\n",
        "    mean_train_mae = train_mae_scores.mean()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CROSS-VALIDATION RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nMean CV MAE: {mean_cv_mae:.4f}\")\n",
        "    print(f\"Std CV MAE: {std_cv_mae:.4f}\")\n",
        "    print(f\"Mean Train MAE: {mean_train_mae:.4f}\")\n",
        "\n",
        "    print(f\"\\nFold-by-fold CV MAE:\")\n",
        "    for fold, mae in enumerate(cv_mae_scores, 1):\n",
        "        print(f\"  Fold {fold}: {mae:.4f}\")\n",
        "\n",
        "    # Train final model on full training set\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Training final model on full training set...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on training set\n",
        "    y_train_pred = pipeline.predict(X_train)\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_test_pred = pipeline.predict(X_test)\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL MODEL EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nCross-Validation MAE: {mean_cv_mae:.4f} ± {std_cv_mae:.4f}\")\n",
        "    print(f\"Training MAE: {train_mae:.4f}\")\n",
        "    print(f\"Test MAE: {test_mae:.4f}\")\n",
        "    print(f\"Difference (Test - Train): {test_mae - train_mae:.4f}\")\n",
        "\n",
        "    if test_mae - train_mae > 0.5:\n",
        "        print(\"\\n⚠️ Warning: Large gap between train and test MAE (possible overfitting)\")\n",
        "    elif test_mae - train_mae < -0.1:\n",
        "        print(\"\\n⚠️ Warning: Test MAE is better than train MAE (check data)\")\n",
        "    else:\n",
        "        print(\"\\n✓ Model generalization looks good\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'model': pipeline,\n",
        "        'mean_cv_mae': mean_cv_mae,\n",
        "        'std_cv_mae': std_cv_mae,\n",
        "        'mean_train_mae': mean_train_mae,\n",
        "        'cv_scores': cv_mae_scores,\n",
        "        'train_scores': train_mae_scores,\n",
        "        'train_mae': train_mae,\n",
        "        'test_mae': test_mae,\n",
        "        'y_train_pred': y_train_pred,\n",
        "        'y_test_pred': y_test_pred,\n",
        "        'cv_results': cv_results\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "t86JAxI2x9Aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t86JAxI2x9Aa",
        "outputId": "ad77b04e-79ae-4dc2-b7b4-a005d462f89f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Linear Regression with Cross-Validation and Scaling\n",
            "Cross-validation folds: 5\n",
            "\n",
            "Running cross-validation on training data...\n",
            "\n",
            "======================================================================\n",
            "CROSS-VALIDATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "Mean CV MAE: 0.1258\n",
            "Std CV MAE: 0.0098\n",
            "Mean Train MAE: 0.1194\n",
            "\n",
            "Fold-by-fold CV MAE:\n",
            "  Fold 1: 0.1247\n",
            "  Fold 2: 0.1157\n",
            "  Fold 3: 0.1181\n",
            "  Fold 4: 0.1267\n",
            "  Fold 5: 0.1435\n",
            "\n",
            "======================================================================\n",
            "Training final model on full training set...\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation MAE: 0.1258 ± 0.0098\n",
            "Training MAE: 0.1202\n",
            "Test MAE: 0.1203\n",
            "Difference (Test - Train): 0.0002\n",
            "\n",
            "✓ Model generalization looks good\n",
            "\n",
            "======================================================================\n",
            "0.12575505648400234\n",
            "Test MAE: 0.1203\n",
            "Train MAE: 0.1202\n",
            "Difference (Test - Train): 0.0002\n"
          ]
        }
      ],
      "source": [
        "results = linear_regression_cv(X_train, y_train, X_test, y_test, cv=5)\n",
        "model = results['model']\n",
        "mean_cv_mae = results['mean_cv_mae']\n",
        "test_mae = results['test_mae']\n",
        "train_MAE = results['train_mae']\n",
        "print(mean_cv_mae)\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Train MAE: {train_MAE:.4f}\")\n",
        "print(f\"Difference (Test - Train): {test_mae - train_MAE:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pUKmRteAWw_K",
      "metadata": {
        "id": "pUKmRteAWw_K"
      },
      "source": [
        "Polynomial Regression with ridge\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "hCYcAb5ob2wV",
      "metadata": {
        "id": "hCYcAb5ob2wV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "def polynomial_ridge_regression_grid_search(X_train, y_train, max_degree=10, cv=5):\n",
        "    \"\"\"\n",
        "    Perform grid search with cross-validation for polynomial regression with Ridge.\n",
        "    Model selection is based ONLY on training data with cross-validation.\n",
        "\n",
        "    Parameters:\n",
        "    X_train : array-like, training features\n",
        "    y_train : array-like, training target\n",
        "    max_degree : int, maximum polynomial degree to test (default=10)\n",
        "    cv : int, number of cross-validation folds (default=5)\n",
        "\n",
        "    Returns:\n",
        "    dict with best model, best degree, best alpha, and results\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a pipeline: Scaling -> Polynomial Features -> Ridge Regression\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('poly_features', PolynomialFeatures()),\n",
        "        ('ridge_reg', Ridge())\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for polynomial degree and Ridge alpha\n",
        "    degrees = list(range(1, max_degree + 1))\n",
        "    alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    param_grid = {\n",
        "        'poly_features__degree': degrees,\n",
        "        'ridge_reg__alpha': alphas\n",
        "    }\n",
        "\n",
        "    # Calculate total fits for progress tracking\n",
        "    num_degrees = len(degrees)\n",
        "    num_alphas = len(alphas)\n",
        "    total_combinations = num_degrees * num_alphas\n",
        "    total_fits = total_combinations * cv\n",
        "\n",
        "    print(\"Performing grid search with cross-validation on training data only...\")\n",
        "    print(f\"Polynomial degrees: {num_degrees} (1 to {max_degree})\")\n",
        "    print(f\"Ridge alpha values: {num_alphas} {alphas}\")\n",
        "    print(f\"Cross-validation folds: {cv}\")\n",
        "    print(f\"Total parameter combinations: {total_combinations}\")\n",
        "    print(f\"Total model fits: {total_fits}\\n\")\n",
        "\n",
        "    # Grid search with cross-validation using MAE\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=cv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        n_jobs=1,  # Set to 1 for progress tracking\n",
        "        verbose=2  # Verbose level 1 shows progress\n",
        "    )\n",
        "\n",
        "    # Fit the grid search\n",
        "    print(\"Starting grid search...\\n\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Extract results\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    results_df['degree'] = results_df['param_poly_features__degree']\n",
        "    results_df['alpha'] = results_df['param_ridge_reg__alpha']\n",
        "    results_df['mean_mae'] = -results_df['mean_test_score']\n",
        "    results_df['std_mae'] = results_df['std_test_score']\n",
        "\n",
        "    # Get best model and parameters\n",
        "    best_degree = grid_search.best_params_['poly_features__degree']\n",
        "    best_alpha = grid_search.best_params_['ridge_reg__alpha']\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_cv_mae = -grid_search.best_score_\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GRID SEARCH RESULTS (Based on Cross-Validation)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBest polynomial degree: {best_degree}\")\n",
        "    print(f\"Best Ridge alpha: {best_alpha}\")\n",
        "    print(f\"Best cross-validation MAE: {best_cv_mae:.4f}\")\n",
        "\n",
        "    print(\"\\nTop 10 combinations ranked by cross-validation MAE:\")\n",
        "    display_results = results_df[['degree', 'alpha', 'mean_mae', 'std_mae']].sort_values('mean_mae').head(10)\n",
        "    print(display_results.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'best_model': best_model,\n",
        "        'best_degree': best_degree,\n",
        "        'best_alpha': best_alpha,\n",
        "        'best_cv_mae': best_cv_mae,\n",
        "        'results_df': results_df,\n",
        "        'grid_search': grid_search\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "H-c4Dh08fclA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-c4Dh08fclA",
        "outputId": "43f059fb-2b7e-4df9-c271-5e93e06983f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing grid search with cross-validation on training data only...\n",
            "Polynomial degrees: 4 (1 to 4)\n",
            "Ridge alpha values: 7 [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
            "Cross-validation folds: 5\n",
            "Total parameter combinations: 28\n",
            "Total model fits: 140\n",
            "\n",
            "Starting grid search...\n",
            "\n",
            "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
            "[CV] END ....poly_features__degree=1, ridge_reg__alpha=0.001; total time=   0.0s\n",
            "[CV] END ....poly_features__degree=1, ridge_reg__alpha=0.001; total time=   0.0s\n",
            "[CV] END ....poly_features__degree=1, ridge_reg__alpha=0.001; total time=   0.0s\n",
            "[CV] END ....poly_features__degree=1, ridge_reg__alpha=0.001; total time=   0.0s\n",
            "[CV] END ....poly_features__degree=1, ridge_reg__alpha=0.001; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=0.1; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=0.1; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=0.1; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=0.1; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=0.1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=1, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=1, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=1, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=1, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=1, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=1, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=1, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=1, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=1, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=1, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=100; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=100; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=100; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=100; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=1, ridge_reg__alpha=100; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=1000; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=1000; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=1000; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=1000; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=1, ridge_reg__alpha=1000; total time=   0.0s\n",
            "[CV] END ....poly_features__degree=2, ridge_reg__alpha=0.001; total time=   0.2s\n",
            "[CV] END ....poly_features__degree=2, ridge_reg__alpha=0.001; total time=   0.0s\n",
            "[CV] END ....poly_features__degree=2, ridge_reg__alpha=0.001; total time=   0.1s\n",
            "[CV] END ....poly_features__degree=2, ridge_reg__alpha=0.001; total time=   0.1s\n",
            "[CV] END ....poly_features__degree=2, ridge_reg__alpha=0.001; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=0.01; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=0.01; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=0.1; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=0.1; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=0.1; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=0.1; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=0.1; total time=   0.1s\n",
            "[CV] END ........poly_features__degree=2, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=2, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=2, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=2, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END ........poly_features__degree=2, ridge_reg__alpha=1; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=2, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=2, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=2, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=2, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END .......poly_features__degree=2, ridge_reg__alpha=10; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=100; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=100; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=100; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=100; total time=   0.0s\n",
            "[CV] END ......poly_features__degree=2, ridge_reg__alpha=100; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=1000; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=1000; total time=   0.0s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=1000; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=1000; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=2, ridge_reg__alpha=1000; total time=   0.0s\n",
            "[CV] END ....poly_features__degree=3, ridge_reg__alpha=0.001; total time=   0.1s\n",
            "[CV] END ....poly_features__degree=3, ridge_reg__alpha=0.001; total time=   0.1s\n",
            "[CV] END ....poly_features__degree=3, ridge_reg__alpha=0.001; total time=   0.1s\n",
            "[CV] END ....poly_features__degree=3, ridge_reg__alpha=0.001; total time=   0.2s\n",
            "[CV] END ....poly_features__degree=3, ridge_reg__alpha=0.001; total time=   0.2s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=0.01; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=0.01; total time=   0.2s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=0.01; total time=   0.3s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=0.01; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=0.01; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=0.1; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=0.1; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=0.1; total time=   0.2s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=0.1; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=0.1; total time=   0.1s\n",
            "[CV] END ........poly_features__degree=3, ridge_reg__alpha=1; total time=   0.1s\n",
            "[CV] END ........poly_features__degree=3, ridge_reg__alpha=1; total time=   0.1s\n",
            "[CV] END ........poly_features__degree=3, ridge_reg__alpha=1; total time=   0.1s\n",
            "[CV] END ........poly_features__degree=3, ridge_reg__alpha=1; total time=   0.1s\n",
            "[CV] END ........poly_features__degree=3, ridge_reg__alpha=1; total time=   0.1s\n",
            "[CV] END .......poly_features__degree=3, ridge_reg__alpha=10; total time=   0.1s\n",
            "[CV] END .......poly_features__degree=3, ridge_reg__alpha=10; total time=   0.1s\n",
            "[CV] END .......poly_features__degree=3, ridge_reg__alpha=10; total time=   0.1s\n",
            "[CV] END .......poly_features__degree=3, ridge_reg__alpha=10; total time=   0.1s\n",
            "[CV] END .......poly_features__degree=3, ridge_reg__alpha=10; total time=   0.6s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=100; total time=   0.2s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=100; total time=   0.3s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=100; total time=   0.1s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=100; total time=   0.3s\n",
            "[CV] END ......poly_features__degree=3, ridge_reg__alpha=100; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=1000; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=1000; total time=   0.2s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=1000; total time=   0.2s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=1000; total time=   0.1s\n",
            "[CV] END .....poly_features__degree=3, ridge_reg__alpha=1000; total time=   0.2s\n",
            "[CV] END ....poly_features__degree=4, ridge_reg__alpha=0.001; total time=   0.7s\n",
            "[CV] END ....poly_features__degree=4, ridge_reg__alpha=0.001; total time=   0.6s\n",
            "[CV] END ....poly_features__degree=4, ridge_reg__alpha=0.001; total time=   0.4s\n",
            "[CV] END ....poly_features__degree=4, ridge_reg__alpha=0.001; total time=   0.3s\n",
            "[CV] END ....poly_features__degree=4, ridge_reg__alpha=0.001; total time=   0.4s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=0.01; total time=   0.4s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=0.01; total time=   0.5s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=0.01; total time=   0.5s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=0.01; total time=   0.9s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=0.01; total time=   0.7s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=0.1; total time=   0.4s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=0.1; total time=   0.4s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=0.1; total time=   0.5s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=0.1; total time=   0.4s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=0.1; total time=   0.4s\n",
            "[CV] END ........poly_features__degree=4, ridge_reg__alpha=1; total time=   0.4s\n",
            "[CV] END ........poly_features__degree=4, ridge_reg__alpha=1; total time=   0.3s\n",
            "[CV] END ........poly_features__degree=4, ridge_reg__alpha=1; total time=   0.4s\n",
            "[CV] END ........poly_features__degree=4, ridge_reg__alpha=1; total time=   0.4s\n",
            "[CV] END ........poly_features__degree=4, ridge_reg__alpha=1; total time=   0.3s\n",
            "[CV] END .......poly_features__degree=4, ridge_reg__alpha=10; total time=   0.3s\n",
            "[CV] END .......poly_features__degree=4, ridge_reg__alpha=10; total time=   0.3s\n",
            "[CV] END .......poly_features__degree=4, ridge_reg__alpha=10; total time=   0.4s\n",
            "[CV] END .......poly_features__degree=4, ridge_reg__alpha=10; total time=   0.5s\n",
            "[CV] END .......poly_features__degree=4, ridge_reg__alpha=10; total time=   0.4s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=100; total time=   0.3s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=100; total time=   0.3s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=100; total time=   0.3s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=100; total time=   0.3s\n",
            "[CV] END ......poly_features__degree=4, ridge_reg__alpha=100; total time=   0.5s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=1000; total time=   0.5s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=1000; total time=   0.4s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=1000; total time=   0.4s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=1000; total time=   0.4s\n",
            "[CV] END .....poly_features__degree=4, ridge_reg__alpha=1000; total time=   0.4s\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH RESULTS (Based on Cross-Validation)\n",
            "======================================================================\n",
            "\n",
            "Best polynomial degree: 2\n",
            "Best Ridge alpha: 10\n",
            "Best cross-validation MAE: 0.1004\n",
            "\n",
            "Top 10 combinations ranked by cross-validation MAE:\n",
            " degree    alpha  mean_mae  std_mae\n",
            "      2   10.000  0.100406 0.010476\n",
            "      2    1.000  0.103271 0.009049\n",
            "      2    0.100  0.105810 0.010137\n",
            "      2  100.000  0.105997 0.012853\n",
            "      3  100.000  0.106154 0.012590\n",
            "      2    0.010  0.111226 0.010094\n",
            "      3 1000.000  0.112451 0.009729\n",
            "      2    0.001  0.115689 0.010603\n",
            "      3   10.000  0.121860 0.013117\n",
            "      1    0.001  0.125785 0.009750\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "results = polynomial_ridge_regression_grid_search(X_train, y_train, max_degree=4, cv=5)\n",
        "ridge_best_model = results['best_model']\n",
        "best_degree = results['best_degree']\n",
        "best_cv_mae = results['best_cv_mae']\n",
        "results_df = results['results_df']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "xdRgL3GwhyOq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdRgL3GwhyOq",
        "outputId": "f3ff968b-cc3a-4acd-942d-4ca63210036d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on training and test data...\n",
            "Done!\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation MAE: 0.1004\n",
            "Training MAE: 0.0794\n",
            "Test MAE: 0.1007\n",
            "Difference (Test - Train): 0.0213\n",
            "\n",
            "✓ Model generalization looks good\n"
          ]
        }
      ],
      "source": [
        "# grid search auto scale and retrains over whole training set and comes up with best_model\n",
        "\n",
        "# ============ Make predictions ============\n",
        "print(\"Making predictions on training and test data...\")\n",
        "y_train_pred = ridge_best_model.predict(X_train)\n",
        "y_test_pred = ridge_best_model.predict(X_test)\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# ============ Calculate MAE ============\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# ============ Print results ============\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCross-Validation MAE: {best_cv_mae:.4f}\")\n",
        "print(f\"Training MAE: {train_mae:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Difference (Test - Train): {test_mae - train_mae:.4f}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "if test_mae - train_mae > 0.5:\n",
        "    print(\"\\n⚠️ Warning: Large gap between train and test MAE (possible overfitting)\")\n",
        "elif test_mae - train_mae < -0.1:\n",
        "    print(\"\\n⚠️ Warning: Test MAE is better than train MAE (check data)\")\n",
        "else:\n",
        "    print(\"\\n✓ Model generalization looks good\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HQr3tQrdiPRM",
      "metadata": {
        "id": "HQr3tQrdiPRM"
      },
      "source": [
        "Polynomial Regression with lasso\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "uUq08wUhiPBq",
      "metadata": {
        "id": "uUq08wUhiPBq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "def polynomial_lasso_regression_grid_search(X_train, y_train, max_degree=10, cv=5):\n",
        "    \"\"\"\n",
        "    Perform grid search with cross-validation for polynomial regression with Lasso.\n",
        "    Model selection is based ONLY on training data with cross-validation.\n",
        "\n",
        "    Parameters:\n",
        "    X_train : array-like, training features\n",
        "    y_train : array-like, training target\n",
        "    max_degree : int, maximum polynomial degree to test (default=10)\n",
        "    cv : int, number of cross-validation folds (default=5)\n",
        "\n",
        "    Returns:\n",
        "    dict with best model, best degree, best alpha, and results\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a pipeline: Scaling -> Polynomial Features -> Lasso Regression\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('poly_features', PolynomialFeatures()),\n",
        "        ('lasso_reg', Lasso(max_iter=10000, tol=0.01, warm_start=True))  # Increased max_iter for convergence\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for polynomial degree and Lasso alpha\n",
        "    degrees = list(range(1, max_degree + 1))\n",
        "    alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "    param_grid = {\n",
        "        'poly_features__degree': degrees,\n",
        "        'lasso_reg__alpha': alphas\n",
        "    }\n",
        "\n",
        "    # Calculate total fits for progress tracking\n",
        "    num_degrees = len(degrees)\n",
        "    num_alphas = len(alphas)\n",
        "    total_combinations = num_degrees * num_alphas\n",
        "    total_fits = total_combinations * cv\n",
        "\n",
        "    print(\"Performing grid search with cross-validation on training data only...\")\n",
        "    print(f\"Polynomial degrees: {num_degrees} (1 to {max_degree})\")\n",
        "    print(f\"Lasso alpha values: {num_alphas} {alphas}\")\n",
        "    print(f\"Cross-validation folds: {cv}\")\n",
        "    print(f\"Total parameter combinations: {total_combinations}\")\n",
        "    print(f\"Total model fits: {total_fits}\\n\")\n",
        "\n",
        "    # Grid search with cross-validation using MAE\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=cv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        n_jobs=1,  # Set to 1 for progress tracking\n",
        "        verbose=2  # Verbose level 1 shows progress\n",
        "    )\n",
        "\n",
        "    # Fit the grid search\n",
        "    print(\"Starting grid search...\\n\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Extract results\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    results_df['degree'] = results_df['param_poly_features__degree']\n",
        "    results_df['alpha'] = results_df['param_lasso_reg__alpha']\n",
        "    results_df['mean_mae'] = -results_df['mean_test_score']\n",
        "    results_df['std_mae'] = results_df['std_test_score']\n",
        "\n",
        "    # Get best model and parameters\n",
        "    best_degree = grid_search.best_params_['poly_features__degree']\n",
        "    best_alpha = grid_search.best_params_['lasso_reg__alpha']\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_cv_mae = -grid_search.best_score_\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GRID SEARCH RESULTS (Based on Cross-Validation)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBest polynomial degree: {best_degree}\")\n",
        "    print(f\"Best Lasso alpha: {best_alpha}\")\n",
        "    print(f\"Best cross-validation MAE: {best_cv_mae:.4f}\")\n",
        "\n",
        "    print(\"\\nTop 10 combinations ranked by cross-validation MAE:\")\n",
        "    display_results = results_df[['degree', 'alpha', 'mean_mae', 'std_mae']].sort_values('mean_mae').head(10)\n",
        "    print(display_results.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'best_model': best_model,\n",
        "        'best_degree': best_degree,\n",
        "        'best_alpha': best_alpha,\n",
        "        'best_cv_mae': best_cv_mae,\n",
        "        'results_df': results_df,\n",
        "        'grid_search': grid_search\n",
        "    }\n",
        "\n",
        "\n",
        "#\n",
        "# # ONLY AFTER selecting the model, evaluate on test set:\n",
        "# y_test_pred = best_model.predict(X_test)\n",
        "# test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "# print(f\"Test set MAE: {test_mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "XUkn-1N6mQ31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUkn-1N6mQ31",
        "outputId": "f6507d23-51ff-4875-f820-569a8658b040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing grid search with cross-validation on training data only...\n",
            "Polynomial degrees: 3 (1 to 3)\n",
            "Lasso alpha values: 7 [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
            "Cross-validation folds: 5\n",
            "Total parameter combinations: 21\n",
            "Total model fits: 105\n",
            "\n",
            "Starting grid search...\n",
            "\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=3; total time=   0.4s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=3; total time=   0.3s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=3; total time=   0.3s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=3; total time=   1.1s\n",
            "[CV] END ....lasso_reg__alpha=0.001, poly_features__degree=3; total time=   0.9s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=3; total time=   0.2s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END .....lasso_reg__alpha=0.01, poly_features__degree=3; total time=   0.2s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END ......lasso_reg__alpha=0.1, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ........lasso_reg__alpha=1, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END .......lasso_reg__alpha=10, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END ......lasso_reg__alpha=100, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END .....lasso_reg__alpha=1000, poly_features__degree=3; total time=   0.0s\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH RESULTS (Based on Cross-Validation)\n",
            "======================================================================\n",
            "\n",
            "Best polynomial degree: 3\n",
            "Best Lasso alpha: 0.001\n",
            "Best cross-validation MAE: 0.0945\n",
            "\n",
            "Top 10 combinations ranked by cross-validation MAE:\n",
            " degree  alpha  mean_mae  std_mae\n",
            "      3  0.001  0.094474 0.009402\n",
            "      2  0.001  0.096951 0.010053\n",
            "      3  0.010  0.105463 0.008577\n",
            "      2  0.010  0.111381 0.014074\n",
            "      1  0.001  0.135286 0.009488\n",
            "      1  0.010  0.143420 0.011122\n",
            "      3  0.100  0.195467 0.009640\n",
            "      2  0.100  0.215557 0.009889\n",
            "      1  0.100  0.223865 0.010352\n",
            "      3  1.000  0.248596 0.010103\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "results = polynomial_lasso_regression_grid_search(X_train, y_train, max_degree=3, cv=5)\n",
        "lasso_best_model = results['best_model']\n",
        "best_degree = results['best_degree']\n",
        "best_alpha = results['best_alpha']\n",
        "best_cv_mae = results['best_cv_mae']\n",
        "results_df = results['results_df']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "VY1hXMILW1R8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY1hXMILW1R8",
        "outputId": "73f6ede8-0eb2-4e5e-85a5-de1d07b17028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on training and test data...\n",
            "Done!\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation MAE: 0.0945\n",
            "Training MAE: 0.0589\n",
            "Test MAE: 0.0925\n",
            "Difference (Test - Train): 0.0336\n",
            "\n",
            "✓ Model generalization looks good\n"
          ]
        }
      ],
      "source": [
        "# grid search auto scale and retrains over whole training set and comes up with best_model\n",
        "\n",
        "# ============ Make predictions ============\n",
        "print(\"Making predictions on training and test data...\")\n",
        "y_train_pred = lasso_best_model.predict(X_train)\n",
        "y_test_pred = lasso_best_model.predict(X_test)\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# ============ Calculate MAE ============\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# ============ Print results ============\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCross-Validation MAE: {best_cv_mae:.4f}\")\n",
        "print(f\"Training MAE: {train_mae:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Difference (Test - Train): {test_mae - train_mae:.4f}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "if test_mae - train_mae > 0.5:\n",
        "    print(\"\\n⚠️ Warning: Large gap between train and test MAE (possible overfitting)\")\n",
        "elif test_mae - train_mae < -0.1:\n",
        "    print(\"\\n⚠️ Warning: Test MAE is better than train MAE (check data)\")\n",
        "else:\n",
        "    print(\"\\n✓ Model generalization looks good\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_gMGaEfDiRTp",
      "metadata": {
        "id": "_gMGaEfDiRTp"
      },
      "source": [
        "Polynomial Regression with elastic net\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "XEDy11ivjB9P",
      "metadata": {
        "id": "XEDy11ivjB9P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "def polynomial_elastic_regression_grid_search(X_train, y_train, max_degree=10, cv=5):\n",
        "    \"\"\"\n",
        "    Perform grid search with cross-validation for polynomial regression with ElasticNet.\n",
        "    Model selection is based ONLY on training data with cross-validation.\n",
        "\n",
        "    Parameters:\n",
        "    X_train : array-like, training features\n",
        "    y_train : array-like, training target\n",
        "    max_degree : int, maximum polynomial degree to test (default=10)\n",
        "    cv : int, number of cross-validation folds (default=5)\n",
        "\n",
        "    Returns:\n",
        "    dict with best model, best degree, best alpha, best l1_ratio, and results\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a pipeline: Scaling -> Polynomial Features -> ElasticNet Regression\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('poly_features', PolynomialFeatures()),\n",
        "        ('elasticnet_reg', ElasticNet(max_iter=10000, tol=0.01, warm_start=True))\n",
        "    ])\n",
        "\n",
        "    # Define parameter grid for polynomial degree, ElasticNet alpha, and l1_ratio\n",
        "    degrees = list(range(1, max_degree + 1))\n",
        "    alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "    l1_ratios = [0.25, 0.5, 0.75]  # 0=Ridge, 1=Lasso, 0-1=ElasticNet\n",
        "\n",
        "    param_grid = {\n",
        "        'poly_features__degree': degrees,\n",
        "        'elasticnet_reg__alpha': alphas,\n",
        "        'elasticnet_reg__l1_ratio': l1_ratios\n",
        "    }\n",
        "\n",
        "    # Calculate total fits for progress tracking\n",
        "    num_degrees = len(degrees)\n",
        "    num_alphas = len(alphas)\n",
        "    num_l1_ratios = len(l1_ratios)\n",
        "    total_combinations = num_degrees * num_alphas * num_l1_ratios\n",
        "    total_fits = total_combinations * cv\n",
        "\n",
        "    print(\"Performing grid search with cross-validation on training data only...\")\n",
        "    print(f\"Polynomial degrees: {num_degrees} (1 to {max_degree})\")\n",
        "    print(f\"ElasticNet alpha values: {num_alphas} {alphas}\")\n",
        "    print(f\"ElasticNet l1_ratio values: {num_l1_ratios} {l1_ratios}\")\n",
        "    print(f\"Cross-validation folds: {cv}\")\n",
        "    print(f\"Total parameter combinations: {total_combinations}\")\n",
        "    print(f\"Total model fits: {total_fits}\\n\")\n",
        "\n",
        "    # Grid search with cross-validation using MAE\n",
        "    grid_search = GridSearchCV(\n",
        "        pipeline,\n",
        "        param_grid,\n",
        "        cv=cv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        n_jobs=1,  # Set to 1 for progress tracking\n",
        "        verbose=2  # Verbose level 1 shows progress\n",
        "    )\n",
        "\n",
        "    # Fit the grid search\n",
        "    print(\"Starting grid search...\\n\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Extract results\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    results_df['degree'] = results_df['param_poly_features__degree']\n",
        "    results_df['alpha'] = results_df['param_elasticnet_reg__alpha']\n",
        "    results_df['l1_ratio'] = results_df['param_elasticnet_reg__l1_ratio']\n",
        "    results_df['mean_mae'] = -results_df['mean_test_score']\n",
        "    results_df['std_mae'] = results_df['std_test_score']\n",
        "\n",
        "    # Get best model and parameters\n",
        "    best_degree = grid_search.best_params_['poly_features__degree']\n",
        "    best_alpha = grid_search.best_params_['elasticnet_reg__alpha']\n",
        "    best_l1_ratio = grid_search.best_params_['elasticnet_reg__l1_ratio']\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_cv_mae = -grid_search.best_score_\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GRID SEARCH RESULTS (Based on Cross-Validation)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBest polynomial degree: {best_degree}\")\n",
        "    print(f\"Best ElasticNet alpha: {best_alpha}\")\n",
        "    print(f\"Best ElasticNet l1_ratio: {best_l1_ratio}\")\n",
        "    print(f\"Best cross-validation MAE: {best_cv_mae:.4f}\")\n",
        "\n",
        "    print(\"\\nTop 10 combinations ranked by cross-validation MAE:\")\n",
        "    display_results = results_df[['degree', 'alpha', 'l1_ratio', 'mean_mae', 'std_mae']].sort_values('mean_mae').head(10)\n",
        "    print(display_results.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'best_model': best_model,\n",
        "        'best_degree': best_degree,\n",
        "        'best_alpha': best_alpha,\n",
        "        'best_l1_ratio': best_l1_ratio,\n",
        "        'best_cv_mae': best_cv_mae,\n",
        "        'results_df': results_df,\n",
        "        'grid_search': grid_search\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "KHsVl1VjjCpe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHsVl1VjjCpe",
        "outputId": "8f22039c-9ced-4ba1-c1bc-bfdc066335f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing grid search with cross-validation on training data only...\n",
            "Polynomial degrees: 3 (1 to 3)\n",
            "ElasticNet alpha values: 7 [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
            "ElasticNet l1_ratio values: 3 [0.25, 0.5, 0.75]\n",
            "Cross-validation folds: 5\n",
            "Total parameter combinations: 63\n",
            "Total model fits: 315\n",
            "\n",
            "Starting grid search...\n",
            "\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   3.7s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   3.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   1.6s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   1.6s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   1.2s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.4s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   1.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   1.2s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   1.9s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.9s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   1.2s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   1.4s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.6s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.9s\n",
            "[CV] END elasticnet_reg__alpha=0.001, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.4s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.2s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.2s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.2s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.2s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.01, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.1s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=0.1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=10, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=100, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.25, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.5, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=1; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=2; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "[CV] END elasticnet_reg__alpha=1000, elasticnet_reg__l1_ratio=0.75, poly_features__degree=3; total time=   0.0s\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH RESULTS (Based on Cross-Validation)\n",
            "======================================================================\n",
            "\n",
            "Best polynomial degree: 3\n",
            "Best ElasticNet alpha: 0.01\n",
            "Best ElasticNet l1_ratio: 0.25\n",
            "Best cross-validation MAE: 0.0939\n",
            "\n",
            "Top 10 combinations ranked by cross-validation MAE:\n",
            " degree  alpha  l1_ratio  mean_mae  std_mae\n",
            "      3  0.010      0.25  0.093946 0.007146\n",
            "      3  0.001      0.75  0.097301 0.010096\n",
            "      2  0.001      0.75  0.097482 0.009800\n",
            "      3  0.010      0.50  0.097865 0.008366\n",
            "      2  0.001      0.50  0.098568 0.009566\n",
            "      2  0.010      0.25  0.098805 0.011446\n",
            "      2  0.001      0.25  0.099797 0.009260\n",
            "      3  0.010      0.75  0.101116 0.008782\n",
            "      3  0.001      0.50  0.103386 0.010570\n",
            "      2  0.010      0.50  0.103801 0.013513\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "results = polynomial_elastic_regression_grid_search(X_train, y_train, max_degree=3, cv=5)\n",
        "elastic_best_model = results['best_model']\n",
        "best_degree = results['best_degree']\n",
        "best_alpha = results['best_alpha']\n",
        "best_l1_ratio = results['best_l1_ratio']\n",
        "best_cv_mae = results['best_cv_mae']\n",
        "results_df = results['results_df']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "69bw3PjdjCr2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69bw3PjdjCr2",
        "outputId": "0373e991-9a6a-434a-afc6-0d603e4317e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on training and test data...\n",
            "Done!\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation MAE: 0.0939\n",
            "Training MAE: 0.0709\n",
            "Test MAE: 0.0957\n",
            "Difference (Test - Train): 0.0247\n",
            "\n",
            "✓ Model generalization looks good\n"
          ]
        }
      ],
      "source": [
        "# grid search auto scale and retrains over whole training set and comes up with best_model\n",
        "\n",
        "# ============ Make predictions ============\n",
        "print(\"Making predictions on training and test data...\")\n",
        "y_train_pred = elastic_best_model.predict(X_train)\n",
        "y_test_pred = elastic_best_model.predict(X_test)\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# ============ Calculate MAE ============\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# ============ Print results ============\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCross-Validation MAE: {best_cv_mae:.4f}\")\n",
        "print(f\"Training MAE: {train_mae:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Difference (Test - Train): {test_mae - train_mae:.4f}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "if test_mae - train_mae > 0.5:\n",
        "    print(\"\\n⚠️ Warning: Large gap between train and test MAE (possible overfitting)\")\n",
        "elif test_mae - train_mae < -0.1:\n",
        "    print(\"\\n⚠️ Warning: Test MAE is better than train MAE (check data)\")\n",
        "else:\n",
        "    print(\"\\n✓ Model generalization looks good\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "grfZOpR5YlvH",
      "metadata": {
        "id": "grfZOpR5YlvH"
      },
      "source": [
        "We basically see that for polynomial regression with various forms of regularisation techniques, they all perform similar to one another, hence we need to look to other methods to possibly improve the MAE. Next we try random forest regression."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y2IaaYF7Y2XJ",
      "metadata": {
        "id": "Y2IaaYF7Y2XJ"
      },
      "source": [
        "Random Forest Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9dv1lkptjCuR",
      "metadata": {
        "id": "9dv1lkptjCuR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "def random_forest_grid_search(X_train, y_train, cv=5):\n",
        "    \"\"\"\n",
        "    Perform grid search with cross-validation for Random Forest regression.\n",
        "    Model selection is based ONLY on training data with cross-validation.\n",
        "    Tunes only max_features while keeping other parameters fixed.\n",
        "\n",
        "    Parameters:\n",
        "    X_train : array-like, training features\n",
        "    y_train : array-like, training target\n",
        "    cv : int, number of cross-validation folds (default=5)\n",
        "\n",
        "    Returns:\n",
        "    dict with best model, best parameters, and results\n",
        "    \"\"\"\n",
        "\n",
        "    # Create Random Forest model with fixed parameters\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Define parameter grid - only tuning max_features\n",
        "    param_grid = {\n",
        "        'max_features': list(range(1, 24))\n",
        "    }\n",
        "\n",
        "    # Calculate total fits for progress tracking\n",
        "    num_max_features = len(param_grid['max_features'])\n",
        "    total_combinations = num_max_features\n",
        "    total_fits = total_combinations * cv\n",
        "\n",
        "    print(\"Performing grid search with cross-validation on training data only...\")\n",
        "    print(f\"n_estimators: 100 (fixed)\")\n",
        "    print(f\"max_depth: 20 (fixed)\")\n",
        "    print(f\"min_samples_leaf: 1 (fixed)\")\n",
        "    print(f\"max_features: {num_max_features} {param_grid['max_features']}\")\n",
        "    print(f\"Cross-validation folds: {cv}\")\n",
        "    print(f\"Total parameter combinations: {total_combinations}\")\n",
        "    print(f\"Total model fits: {total_fits}\\n\")\n",
        "\n",
        "    # Grid search with cross-validation using MAE\n",
        "    grid_search = GridSearchCV(\n",
        "        rf_model,\n",
        "        param_grid,\n",
        "        cv=cv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        n_jobs=1,  # Use all cores for speed\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # Fit the grid search\n",
        "    print(\"Starting grid search...\\n\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Extract results\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    results_df['max_features'] = results_df['param_max_features']\n",
        "    results_df['mean_mae'] = -results_df['mean_test_score']\n",
        "    results_df['std_mae'] = results_df['std_test_score']\n",
        "\n",
        "    # Get best model and parameters\n",
        "    best_max_features = grid_search.best_params_['max_features']\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_cv_mae = -grid_search.best_score_\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GRID SEARCH RESULTS (Based on Cross-Validation)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBest max_features: {best_max_features}\")\n",
        "    print(f\"Best cross-validation MAE: {best_cv_mae:.4f}\")\n",
        "\n",
        "    print(\"\\nAll max_features ranked by cross-validation MAE:\")\n",
        "    display_results = results_df[['max_features', 'mean_mae', 'std_mae']].sort_values('mean_mae')\n",
        "    print(display_results.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'best_model': best_model,\n",
        "        'best_max_features': best_max_features,\n",
        "        'best_cv_mae': best_cv_mae,\n",
        "        'results_df': results_df,\n",
        "        'grid_search': grid_search\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "P4VlMBOGjCwo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4VlMBOGjCwo",
        "outputId": "58cd3780-d362-482a-824b-f5679037a61a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing grid search with cross-validation on training data only...\n",
            "n_estimators: 100 (fixed)\n",
            "max_depth: 20 (fixed)\n",
            "min_samples_leaf: 1 (fixed)\n",
            "max_features: 23 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
            "Cross-validation folds: 5\n",
            "Total parameter combinations: 23\n",
            "Total model fits: 115\n",
            "\n",
            "Starting grid search...\n",
            "\n",
            "Fitting 5 folds for each of 23 candidates, totalling 115 fits\n",
            "[CV] END .....................................max_features=1; total time=   0.2s\n",
            "[CV] END .....................................max_features=1; total time=   0.2s\n",
            "[CV] END .....................................max_features=1; total time=   0.2s\n",
            "[CV] END .....................................max_features=1; total time=   0.2s\n",
            "[CV] END .....................................max_features=1; total time=   0.2s\n",
            "[CV] END .....................................max_features=2; total time=   0.2s\n",
            "[CV] END .....................................max_features=2; total time=   0.2s\n",
            "[CV] END .....................................max_features=2; total time=   0.2s\n",
            "[CV] END .....................................max_features=2; total time=   0.2s\n",
            "[CV] END .....................................max_features=2; total time=   0.2s\n",
            "[CV] END .....................................max_features=3; total time=   0.2s\n",
            "[CV] END .....................................max_features=3; total time=   0.3s\n",
            "[CV] END .....................................max_features=3; total time=   0.2s\n",
            "[CV] END .....................................max_features=3; total time=   0.3s\n",
            "[CV] END .....................................max_features=3; total time=   0.2s\n",
            "[CV] END .....................................max_features=4; total time=   0.3s\n",
            "[CV] END .....................................max_features=4; total time=   0.3s\n",
            "[CV] END .....................................max_features=4; total time=   0.3s\n",
            "[CV] END .....................................max_features=4; total time=   0.3s\n",
            "[CV] END .....................................max_features=4; total time=   0.3s\n",
            "[CV] END .....................................max_features=5; total time=   0.3s\n",
            "[CV] END .....................................max_features=5; total time=   0.3s\n",
            "[CV] END .....................................max_features=5; total time=   0.3s\n",
            "[CV] END .....................................max_features=5; total time=   0.3s\n",
            "[CV] END .....................................max_features=5; total time=   0.3s\n",
            "[CV] END .....................................max_features=6; total time=   0.4s\n",
            "[CV] END .....................................max_features=6; total time=   0.4s\n",
            "[CV] END .....................................max_features=6; total time=   0.5s\n",
            "[CV] END .....................................max_features=6; total time=   0.5s\n",
            "[CV] END .....................................max_features=6; total time=   0.5s\n",
            "[CV] END .....................................max_features=7; total time=   0.6s\n",
            "[CV] END .....................................max_features=7; total time=   0.4s\n",
            "[CV] END .....................................max_features=7; total time=   0.4s\n",
            "[CV] END .....................................max_features=7; total time=   0.4s\n",
            "[CV] END .....................................max_features=7; total time=   0.4s\n",
            "[CV] END .....................................max_features=8; total time=   0.4s\n",
            "[CV] END .....................................max_features=8; total time=   0.4s\n",
            "[CV] END .....................................max_features=8; total time=   0.4s\n",
            "[CV] END .....................................max_features=8; total time=   0.4s\n",
            "[CV] END .....................................max_features=8; total time=   0.4s\n",
            "[CV] END .....................................max_features=9; total time=   0.5s\n",
            "[CV] END .....................................max_features=9; total time=   0.5s\n",
            "[CV] END .....................................max_features=9; total time=   0.5s\n",
            "[CV] END .....................................max_features=9; total time=   0.5s\n",
            "[CV] END .....................................max_features=9; total time=   0.5s\n",
            "[CV] END ....................................max_features=10; total time=   0.5s\n",
            "[CV] END ....................................max_features=10; total time=   0.5s\n",
            "[CV] END ....................................max_features=10; total time=   0.5s\n",
            "[CV] END ....................................max_features=10; total time=   0.5s\n",
            "[CV] END ....................................max_features=10; total time=   0.5s\n",
            "[CV] END ....................................max_features=11; total time=   0.6s\n",
            "[CV] END ....................................max_features=11; total time=   0.6s\n",
            "[CV] END ....................................max_features=11; total time=   0.7s\n",
            "[CV] END ....................................max_features=11; total time=   0.8s\n",
            "[CV] END ....................................max_features=11; total time=   0.8s\n",
            "[CV] END ....................................max_features=12; total time=   0.7s\n",
            "[CV] END ....................................max_features=12; total time=   0.6s\n",
            "[CV] END ....................................max_features=12; total time=   0.6s\n",
            "[CV] END ....................................max_features=12; total time=   0.6s\n",
            "[CV] END ....................................max_features=12; total time=   0.6s\n",
            "[CV] END ....................................max_features=13; total time=   0.6s\n",
            "[CV] END ....................................max_features=13; total time=   0.6s\n",
            "[CV] END ....................................max_features=13; total time=   0.7s\n",
            "[CV] END ....................................max_features=13; total time=   0.6s\n",
            "[CV] END ....................................max_features=13; total time=   0.6s\n",
            "[CV] END ....................................max_features=14; total time=   0.7s\n",
            "[CV] END ....................................max_features=14; total time=   0.6s\n",
            "[CV] END ....................................max_features=14; total time=   0.7s\n",
            "[CV] END ....................................max_features=14; total time=   0.7s\n",
            "[CV] END ....................................max_features=14; total time=   0.7s\n",
            "[CV] END ....................................max_features=15; total time=   0.7s\n",
            "[CV] END ....................................max_features=15; total time=   0.9s\n",
            "[CV] END ....................................max_features=15; total time=   0.9s\n",
            "[CV] END ....................................max_features=15; total time=   0.8s\n",
            "[CV] END ....................................max_features=15; total time=   0.7s\n",
            "[CV] END ....................................max_features=16; total time=   0.7s\n",
            "[CV] END ....................................max_features=16; total time=   0.7s\n",
            "[CV] END ....................................max_features=16; total time=   0.7s\n",
            "[CV] END ....................................max_features=16; total time=   0.7s\n",
            "[CV] END ....................................max_features=16; total time=   0.7s\n",
            "[CV] END ....................................max_features=17; total time=   0.8s\n",
            "[CV] END ....................................max_features=17; total time=   0.7s\n",
            "[CV] END ....................................max_features=17; total time=   0.8s\n",
            "[CV] END ....................................max_features=17; total time=   0.8s\n",
            "[CV] END ....................................max_features=17; total time=   0.8s\n",
            "[CV] END ....................................max_features=18; total time=   0.8s\n",
            "[CV] END ....................................max_features=18; total time=   0.8s\n",
            "[CV] END ....................................max_features=18; total time=   1.1s\n",
            "[CV] END ....................................max_features=18; total time=   1.1s\n",
            "[CV] END ....................................max_features=18; total time=   0.8s\n",
            "[CV] END ....................................max_features=19; total time=   0.8s\n",
            "[CV] END ....................................max_features=19; total time=   0.8s\n",
            "[CV] END ....................................max_features=19; total time=   0.8s\n",
            "[CV] END ....................................max_features=19; total time=   0.9s\n",
            "[CV] END ....................................max_features=19; total time=   0.8s\n",
            "[CV] END ....................................max_features=20; total time=   0.9s\n",
            "[CV] END ....................................max_features=20; total time=   0.9s\n",
            "[CV] END ....................................max_features=20; total time=   0.9s\n",
            "[CV] END ....................................max_features=20; total time=   0.9s\n",
            "[CV] END ....................................max_features=20; total time=   0.9s\n",
            "[CV] END ....................................max_features=21; total time=   1.1s\n",
            "[CV] END ....................................max_features=21; total time=   1.3s\n",
            "[CV] END ....................................max_features=21; total time=   1.0s\n",
            "[CV] END ....................................max_features=21; total time=   0.9s\n",
            "[CV] END ....................................max_features=21; total time=   1.0s\n",
            "[CV] END ....................................max_features=22; total time=   0.9s\n",
            "[CV] END ....................................max_features=22; total time=   1.0s\n",
            "[CV] END ....................................max_features=22; total time=   0.9s\n",
            "[CV] END ....................................max_features=22; total time=   1.0s\n",
            "[CV] END ....................................max_features=22; total time=   0.9s\n",
            "[CV] END ....................................max_features=23; total time=   1.0s\n",
            "[CV] END ....................................max_features=23; total time=   1.4s\n",
            "[CV] END ....................................max_features=23; total time=   1.3s\n",
            "[CV] END ....................................max_features=23; total time=   1.3s\n",
            "[CV] END ....................................max_features=23; total time=   1.0s\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH RESULTS (Based on Cross-Validation)\n",
            "======================================================================\n",
            "\n",
            "Best max_features: 19\n",
            "Best cross-validation MAE: 0.0832\n",
            "\n",
            "All max_features ranked by cross-validation MAE:\n",
            " max_features  mean_mae  std_mae\n",
            "           19  0.083192 0.009051\n",
            "           17  0.083722 0.008506\n",
            "           14  0.083724 0.008202\n",
            "           20  0.083824 0.009075\n",
            "           23  0.083831 0.008816\n",
            "           22  0.084057 0.008760\n",
            "           18  0.084087 0.008795\n",
            "           11  0.084232 0.009143\n",
            "           15  0.084284 0.008282\n",
            "           16  0.084364 0.008661\n",
            "           12  0.084642 0.009388\n",
            "            9  0.084871 0.008690\n",
            "           21  0.084980 0.008287\n",
            "           13  0.085022 0.008487\n",
            "           10  0.085285 0.008416\n",
            "            7  0.087234 0.009156\n",
            "            8  0.087283 0.009201\n",
            "            6  0.089090 0.009999\n",
            "            5  0.091799 0.009520\n",
            "            4  0.095112 0.009330\n",
            "            3  0.098484 0.011075\n",
            "            2  0.105659 0.009654\n",
            "            1  0.117459 0.009641\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "results = random_forest_grid_search(X_train, y_train, cv=5)\n",
        "rf_best_model = results['best_model']\n",
        "best_max_features = results['best_max_features']\n",
        "best_cv_mae = results['best_cv_mae']\n",
        "results_df = results['results_df']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "Rt14XnGnSn1u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt14XnGnSn1u",
        "outputId": "28d8c438-84e4-4502-cab5-4602af0459c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on training and test data...\n",
            "Done!\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation MAE: 0.0832\n",
            "Training MAE: 0.0304\n",
            "Test MAE: 0.0808\n",
            "Difference (Test - Train): 0.0503\n",
            "\n",
            "✓ Model generalization looks good\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============ Make predictions ============\n",
        "print(\"Making predictions on training and test data...\")\n",
        "y_train_pred = rf_best_model.predict(X_train)\n",
        "y_test_pred = rf_best_model.predict(X_test)\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# ============ Calculate MAE ============\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# ============ Print results ============\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCross-Validation MAE: {best_cv_mae:.4f}\")\n",
        "print(f\"Training MAE: {train_mae:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Difference (Test - Train): {test_mae - train_mae:.4f}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "if test_mae - train_mae > 0.5:\n",
        "    print(\"\\n⚠️ Warning: Large gap between train and test MAE (possible overfitting)\")\n",
        "elif test_mae - train_mae < -0.1:\n",
        "    print(\"\\n⚠️ Warning: Test MAE is better than train MAE (check data)\")\n",
        "else:\n",
        "    print(\"\\n✓ Model generalization looks good\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AJTU6iAC2C7Q",
      "metadata": {
        "id": "AJTU6iAC2C7Q"
      },
      "source": [
        "Adaptive Boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "-jDrzNw9vRfa",
      "metadata": {
        "id": "-jDrzNw9vRfa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "def adaboost_grid_search(X_train, y_train, cv=5):\n",
        "    \"\"\"\n",
        "    Perform grid search with cross-validation for AdaBoost regression.\n",
        "    Model selection is based ONLY on training data with cross-validation.\n",
        "\n",
        "    Parameters:\n",
        "    X_train : array-like, training features\n",
        "    y_train : array-like, training target\n",
        "    cv : int, number of cross-validation folds (default=5)\n",
        "\n",
        "    Returns:\n",
        "    dict with best model, best parameters, and results\n",
        "    \"\"\"\n",
        "\n",
        "    # Create AdaBoost model with DecisionTree as base estimator\n",
        "    ada_model = AdaBoostRegressor(\n",
        "        estimator=DecisionTreeRegressor(max_depth=5),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200, 300],\n",
        "        'learning_rate': [0.001, 0.001, 0.01, 0.1, 1.0],\n",
        "        'loss': ['linear', 'square', 'exponential']\n",
        "    }\n",
        "\n",
        "    # Calculate total fits for progress tracking\n",
        "    num_n_estimators = len(param_grid['n_estimators'])\n",
        "    num_learning_rate = len(param_grid['learning_rate'])\n",
        "    num_loss = len(param_grid['loss'])\n",
        "    total_combinations = num_n_estimators * num_learning_rate * num_loss\n",
        "    total_fits = total_combinations * cv\n",
        "\n",
        "    print(\"Performing grid search with cross-validation on training data only...\")\n",
        "    print(f\"n_estimators: {num_n_estimators} {param_grid['n_estimators']}\")\n",
        "    print(f\"learning_rate: {num_learning_rate} {param_grid['learning_rate']}\")\n",
        "    print(f\"loss: {num_loss} {param_grid['loss']}\")\n",
        "    print(f\"Cross-validation folds: {cv}\")\n",
        "    print(f\"Total parameter combinations: {total_combinations}\")\n",
        "    print(f\"Total model fits: {total_fits}\\n\")\n",
        "\n",
        "    # Grid search with cross-validation using MAE\n",
        "    grid_search = GridSearchCV(\n",
        "        ada_model,\n",
        "        param_grid,\n",
        "        cv=cv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        n_jobs=1,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # Fit the grid search\n",
        "    print(\"Starting grid search...\\n\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Extract results\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    results_df['n_estimators'] = results_df['param_n_estimators']\n",
        "    results_df['learning_rate'] = results_df['param_learning_rate']\n",
        "    results_df['loss'] = results_df['param_loss']\n",
        "    results_df['mean_mae'] = -results_df['mean_test_score']\n",
        "    results_df['std_mae'] = results_df['std_test_score']\n",
        "\n",
        "    # Get best model and parameters\n",
        "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
        "    best_learning_rate = grid_search.best_params_['learning_rate']\n",
        "    best_loss = grid_search.best_params_['loss']\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_cv_mae = -grid_search.best_score_\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GRID SEARCH RESULTS (Based on Cross-Validation)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBest n_estimators: {best_n_estimators}\")\n",
        "    print(f\"Best learning_rate: {best_learning_rate}\")\n",
        "    print(f\"Best loss: {best_loss}\")\n",
        "    print(f\"Best cross-validation MAE: {best_cv_mae:.4f}\")\n",
        "\n",
        "    print(\"\\nTop 10 combinations ranked by cross-validation MAE:\")\n",
        "    display_results = results_df[['n_estimators', 'learning_rate', 'loss', 'mean_mae', 'std_mae']].sort_values('mean_mae').head(10)\n",
        "    print(display_results.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'best_model': best_model,\n",
        "        'best_n_estimators': best_n_estimators,\n",
        "        'best_learning_rate': best_learning_rate,\n",
        "        'best_loss': best_loss,\n",
        "        'best_cv_mae': best_cv_mae,\n",
        "        'results_df': results_df,\n",
        "        'grid_search': grid_search\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "pNwZMWdNwJEX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNwZMWdNwJEX",
        "outputId": "cbcd79a9-239e-4163-c882-e57289106eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing grid search with cross-validation on training data only...\n",
            "n_estimators: 4 [50, 100, 200, 300]\n",
            "learning_rate: 5 [0.001, 0.001, 0.01, 0.1, 1.0]\n",
            "loss: 3 ['linear', 'square', 'exponential']\n",
            "Cross-validation folds: 5\n",
            "Total parameter combinations: 60\n",
            "Total model fits: 300\n",
            "\n",
            "Starting grid search...\n",
            "\n",
            "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.6s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   1.0s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   1.7s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   2.0s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   1.0s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.5s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.5s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.5s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   1.0s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   1.7s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   2.3s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   3.0s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.0s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.0s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.2s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.5s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.5s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.5s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.7s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.1s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.0s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.0s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.1s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.5s\n",
            "[CV] END ..learning_rate=0.001, loss=linear, n_estimators=50; total time=   0.5s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   0.9s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.7s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.001, loss=square, n_estimators=50; total time=   0.5s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   1.0s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   0.9s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.6s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.3s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.1s\n",
            "[CV] END .learning_rate=0.001, loss=square, n_estimators=300; total time=   2.1s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.9s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   1.0s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.9s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.3s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   3.5s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.2s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.0s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.1s\n",
            "[CV] END learning_rate=0.001, loss=exponential, n_estimators=300; total time=   2.1s\n",
            "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.4s\n",
            "[CV] END ...learning_rate=0.01, loss=linear, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.9s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   1.0s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.9s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=300; total time=   2.3s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=300; total time=   2.5s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=300; total time=   2.0s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=300; total time=   2.1s\n",
            "[CV] END ..learning_rate=0.01, loss=linear, n_estimators=300; total time=   2.0s\n",
            "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.01, loss=square, n_estimators=50; total time=   0.4s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.8s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.9s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.9s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   1.3s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=200; total time=   1.4s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=300; total time=   2.0s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=300; total time=   2.7s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=300; total time=   2.0s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=300; total time=   2.0s\n",
            "[CV] END ..learning_rate=0.01, loss=square, n_estimators=300; total time=   2.0s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.4s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   1.0s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=100; total time=   1.0s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   1.5s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=200; total time=   1.4s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=300; total time=   2.0s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=300; total time=   2.4s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=300; total time=   2.5s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=300; total time=   2.0s\n",
            "[CV] END learning_rate=0.01, loss=exponential, n_estimators=300; total time=   2.0s\n",
            "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.4s\n",
            "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=0.1, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.6s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.6s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=100; total time=   0.8s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   2.1s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   1.2s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   1.2s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   1.2s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=200; total time=   1.3s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=300; total time=   1.8s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=300; total time=   1.8s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=300; total time=   1.9s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=300; total time=   2.3s\n",
            "[CV] END ...learning_rate=0.1, loss=linear, n_estimators=300; total time=   1.8s\n",
            "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.4s\n",
            "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.4s\n",
            "[CV] END ....learning_rate=0.1, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.7s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.6s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.6s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.6s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=100; total time=   0.6s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   1.2s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   1.2s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   1.5s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   1.5s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=200; total time=   1.2s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=300; total time=   1.7s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=300; total time=   1.7s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=300; total time=   1.7s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=300; total time=   1.7s\n",
            "[CV] END ...learning_rate=0.1, loss=square, n_estimators=300; total time=   1.9s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.5s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.5s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.5s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.5s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=50; total time=   0.4s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.6s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.7s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=100; total time=   0.6s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   1.2s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   1.2s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   1.2s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   1.3s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=200; total time=   1.3s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=300; total time=   2.5s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=300; total time=   1.8s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=300; total time=   1.8s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=300; total time=   1.8s\n",
            "[CV] END learning_rate=0.1, loss=exponential, n_estimators=300; total time=   1.8s\n",
            "[CV] END ....learning_rate=1.0, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=1.0, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=1.0, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=1.0, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=1.0, loss=linear, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=100; total time=   0.5s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=100; total time=   0.5s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=100; total time=   0.6s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=100; total time=   0.7s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=200; total time=   1.2s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=200; total time=   1.0s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=200; total time=   1.0s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=200; total time=   1.0s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=200; total time=   1.0s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=300; total time=   1.5s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=300; total time=   1.5s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=300; total time=   1.5s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=300; total time=   1.8s\n",
            "[CV] END ...learning_rate=1.0, loss=linear, n_estimators=300; total time=   1.8s\n",
            "[CV] END ....learning_rate=1.0, loss=square, n_estimators=50; total time=   0.2s\n",
            "[CV] END ....learning_rate=1.0, loss=square, n_estimators=50; total time=   0.2s\n",
            "[CV] END ....learning_rate=1.0, loss=square, n_estimators=50; total time=   0.2s\n",
            "[CV] END ....learning_rate=1.0, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ....learning_rate=1.0, loss=square, n_estimators=50; total time=   0.3s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=100; total time=   0.5s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=100; total time=   0.5s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=100; total time=   0.5s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=100; total time=   0.5s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=100; total time=   0.5s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=200; total time=   0.9s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=200; total time=   0.9s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=200; total time=   0.9s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=200; total time=   0.9s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=200; total time=   0.9s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=300; total time=   1.5s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=300; total time=   1.9s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=300; total time=   1.4s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=300; total time=   1.3s\n",
            "[CV] END ...learning_rate=1.0, loss=square, n_estimators=300; total time=   1.4s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=50; total time=   0.3s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=100; total time=   0.5s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=100; total time=   0.5s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=100; total time=   0.5s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=100; total time=   0.5s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=100; total time=   0.5s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=200; total time=   1.0s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=200; total time=   1.2s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=200; total time=   1.5s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=200; total time=   1.1s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=200; total time=   1.1s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=300; total time=   1.6s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=300; total time=   1.6s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=300; total time=   1.5s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=300; total time=   1.5s\n",
            "[CV] END learning_rate=1.0, loss=exponential, n_estimators=300; total time=   1.5s\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH RESULTS (Based on Cross-Validation)\n",
            "======================================================================\n",
            "\n",
            "Best n_estimators: 300\n",
            "Best learning_rate: 1.0\n",
            "Best loss: square\n",
            "Best cross-validation MAE: 0.0916\n",
            "\n",
            "Top 10 combinations ranked by cross-validation MAE:\n",
            " n_estimators  learning_rate        loss  mean_mae  std_mae\n",
            "          300          1.000      square  0.091554 0.006262\n",
            "          300          0.100      square  0.091713 0.006126\n",
            "          200          1.000      square  0.091951 0.006123\n",
            "          200          0.100 exponential  0.091980 0.006705\n",
            "          100          0.100 exponential  0.092319 0.007139\n",
            "          100          0.001      square  0.092436 0.010222\n",
            "          100          0.001      square  0.092436 0.010222\n",
            "          300          0.010 exponential  0.092490 0.008726\n",
            "          200          0.100      linear  0.092496 0.007082\n",
            "          200          0.100      square  0.092534 0.005887\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "results = adaboost_grid_search(X_train, y_train, cv=5)\n",
        "ada_best_model = results['best_model']\n",
        "best_n_estimators = results['best_n_estimators']\n",
        "best_learning_rate = results['best_learning_rate']\n",
        "best_loss = results['best_loss']\n",
        "best_cv_mae = results['best_cv_mae']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "_jq8aR-bwn3u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jq8aR-bwn3u",
        "outputId": "485b58d7-b883-4727-8e8d-7377d21d5143"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on training and test data...\n",
            "Done!\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation MAE: 0.0916\n",
            "Training MAE: 0.0601\n",
            "Test MAE: 0.0849\n",
            "Difference (Test - Train): 0.0248\n",
            "\n",
            "✓ Model generalization looks good\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============ Make predictions ============\n",
        "print(\"Making predictions on training and test data...\")\n",
        "y_train_pred = ada_best_model.predict(X_train)\n",
        "y_test_pred = ada_best_model.predict(X_test)\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# ============ Calculate MAE ============\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# ============ Print results ============\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCross-Validation MAE: {best_cv_mae:.4f}\")\n",
        "print(f\"Training MAE: {train_mae:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Difference (Test - Train): {test_mae - train_mae:.4f}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "if test_mae - train_mae > 0.5:\n",
        "    print(\"\\n⚠️ Warning: Large gap between train and test MAE (possible overfitting)\")\n",
        "elif test_mae - train_mae < -0.1:\n",
        "    print(\"\\n⚠️ Warning: Test MAE is better than train MAE (check data)\")\n",
        "else:\n",
        "    print(\"\\n✓ Model generalization looks good\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bCnCEyur2IyA",
      "metadata": {
        "id": "bCnCEyur2IyA"
      },
      "source": [
        "Light Gradient Boosting Machine (LGBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TotcNKX52NWb",
      "metadata": {
        "id": "TotcNKX52NWb"
      },
      "outputs": [],
      "source": [
        "# install LGBM\n",
        "# pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3ksOr-ygXzlj",
      "metadata": {
        "id": "3ksOr-ygXzlj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "def lgbm_grid_search(X_train, y_train, cv=5):\n",
        "    \"\"\"\n",
        "    Perform grid search with cross-validation for LightGBM regression.\n",
        "    Model selection is based ONLY on training data with cross-validation.\n",
        "\n",
        "    Parameters:\n",
        "    X_train : array-like, training features\n",
        "    y_train : array-like, training target\n",
        "    cv : int, number of cross-validation folds (default=5)\n",
        "\n",
        "    Returns:\n",
        "    dict with best model, best parameters, and results\n",
        "    \"\"\"\n",
        "\n",
        "    # Create LightGBM model\n",
        "    lgbm_model = lgb.LGBMRegressor(\n",
        "        random_state=42,\n",
        "        verbose=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Define parameter grid: ADDING 'reg_alpha'\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 500],\n",
        "        'learning_rate': [0.001, 0.01, 0.1, 1],\n",
        "        'max_depth': [5, 10, 15, 20, 24]\n",
        "    }\n",
        "\n",
        "    # Calculate total fits for progress tracking\n",
        "    num_n_estimators = len(param_grid['n_estimators'])\n",
        "    num_learning_rate = len(param_grid['learning_rate'])\n",
        "    num_max_depth = len(param_grid['max_depth'])\n",
        "    total_combinations = (num_n_estimators * num_learning_rate * num_max_depth)\n",
        "    total_fits = total_combinations * cv\n",
        "\n",
        "    print(\"Performing grid search with cross-validation on training data only...\")\n",
        "    print(f\"n_estimators: {num_n_estimators} {param_grid['n_estimators']}\")\n",
        "    print(f\"learning_rate: {num_learning_rate} {param_grid['learning_rate']}\")\n",
        "    print(f\"max_depth: {num_max_depth} {param_grid['max_depth']}\")\n",
        "    print(f\"Cross-validation folds: {cv}\")\n",
        "    print(f\"Total parameter combinations: {total_combinations}\")\n",
        "    print(f\"Total model fits: {total_fits}\\n\")\n",
        "\n",
        "    # Grid search with cross-validation using MAE\n",
        "    grid_search = GridSearchCV(\n",
        "        lgbm_model,\n",
        "        param_grid,\n",
        "        cv=cv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Fit the grid search\n",
        "    print(\"Starting grid search...\\n\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Extract results\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    # Extract existing params\n",
        "    results_df['n_estimators'] = results_df['param_n_estimators']\n",
        "    results_df['learning_rate'] = results_df['param_learning_rate']\n",
        "    results_df['max_depth'] = results_df['param_max_depth']\n",
        "    results_df['mean_mae'] = -results_df['mean_test_score']\n",
        "    results_df['std_mae'] = results_df['std_test_score']\n",
        "\n",
        "    # Get best model and parameters\n",
        "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
        "    best_learning_rate = grid_search.best_params_['learning_rate']\n",
        "    best_max_depth = grid_search.best_params_['max_depth']\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_cv_mae = -grid_search.best_score_\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GRID SEARCH RESULTS (Based on Cross-Validation)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBest n_estimators: {best_n_estimators}\")\n",
        "    print(f\"Best learning_rate: {best_learning_rate}\")\n",
        "    print(f\"Best max_depth: {best_max_depth}\")\n",
        "    print(f\"Best cross-validation MAE: {best_cv_mae:.4f}\")\n",
        "\n",
        "    print(\"\\nTop 10 combinations ranked by cross-validation MAE:\")\n",
        "    display_results = results_df[['n_estimators', 'learning_rate',\n",
        "                                  'max_depth', 'mean_mae', 'std_mae']].sort_values('mean_mae').head(10)\n",
        "    print(display_results.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'best_model': best_model,\n",
        "        'best_n_estimators': best_n_estimators,\n",
        "        'best_learning_rate': best_learning_rate,\n",
        "        'best_max_depth': best_max_depth,\n",
        "        'best_cv_mae': best_cv_mae,\n",
        "        'results_df': results_df,\n",
        "        'grid_search': grid_search\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "cgif2N_75Mo7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgif2N_75Mo7",
        "outputId": "c6827b2e-af2c-41e9-918c-3bea7b24e5b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing grid search with cross-validation on training data only...\n",
            "n_estimators: 3 [50, 100, 500]\n",
            "learning_rate: 4 [0.001, 0.01, 0.1, 1]\n",
            "max_depth: 5 [5, 10, 15, 20, 24]\n",
            "Cross-validation folds: 5\n",
            "Total parameter combinations: 60\n",
            "Total model fits: 300\n",
            "\n",
            "Starting grid search...\n",
            "\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH RESULTS (Based on Cross-Validation)\n",
            "======================================================================\n",
            "\n",
            "Best n_estimators: 500\n",
            "Best learning_rate: 0.1\n",
            "Best max_depth: 20\n",
            "Best cross-validation MAE: 0.0817\n",
            "\n",
            "Top 10 combinations ranked by cross-validation MAE:\n",
            " n_estimators  learning_rate  max_depth  mean_mae  std_mae\n",
            "          500            0.1         20  0.081749 0.006732\n",
            "          500            0.1         24  0.081836 0.006743\n",
            "          100            0.1         24  0.081870 0.007482\n",
            "          100            0.1         20  0.081870 0.007482\n",
            "           50            0.1         24  0.081987 0.008858\n",
            "           50            0.1         20  0.081987 0.008858\n",
            "          100            0.1         15  0.082035 0.007809\n",
            "           50            0.1         15  0.082042 0.008927\n",
            "          500            0.1         10  0.082105 0.006749\n",
            "          100            0.1         10  0.082280 0.007177\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "results = lgbm_grid_search(X_train, y_train, cv=5)\n",
        "lgbm_best_model = results['best_model']\n",
        "best_n_estimators = results['best_n_estimators']\n",
        "best_learning_rate = results['best_learning_rate']\n",
        "best_max_depth = results['best_max_depth']\n",
        "best_cv_mae = results['best_cv_mae']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "HLTKTfP85g4M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLTKTfP85g4M",
        "outputId": "86c9112e-ab75-43b0-a6cb-3d27b65e0096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on training and test data...\n",
            "Done!\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation MAE: 0.0817\n",
            "Training MAE: 0.0009\n",
            "Test MAE: 0.0793\n",
            "Difference (Test - Train): 0.0784\n",
            "\n",
            "✓ Model generalization looks good\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============ Make predictions ============\n",
        "print(\"Making predictions on training and test data...\")\n",
        "y_train_pred = lgbm_best_model.predict(X_train)\n",
        "y_test_pred = lgbm_best_model.predict(X_test)\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# ============ Calculate MAE ============\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# ============ Print results ============\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCross-Validation MAE: {best_cv_mae:.4f}\")\n",
        "print(f\"Training MAE: {train_mae:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Difference (Test - Train): {test_mae - train_mae:.4f}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "if test_mae - train_mae > 0.5:\n",
        "    print(\"\\n⚠️ Warning: Large gap between train and test MAE (possible overfitting)\")\n",
        "elif test_mae - train_mae < -0.1:\n",
        "    print(\"\\n⚠️ Warning: Test MAE is better than train MAE (check data)\")\n",
        "else:\n",
        "    print(\"\\n✓ Model generalization looks good\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fhleUZGXx--A",
      "metadata": {
        "id": "fhleUZGXx--A"
      },
      "source": [
        "Extreme Gradient Boost (XGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "j3vTzoACv5sT",
      "metadata": {
        "id": "j3vTzoACv5sT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "\n",
        "def xgb_grid_search(X_train, y_train, cv=5):\n",
        "    \"\"\"\n",
        "    Perform grid search with cross-validation for XGBoost regression.\n",
        "    Model selection is based ONLY on training data with cross-validation.\n",
        "\n",
        "    Parameters:\n",
        "    X_train : array-like, training features\n",
        "    y_train : array-like, training target\n",
        "    cv : int, number of cross-validation folds (default=5)\n",
        "\n",
        "    Returns:\n",
        "    dict with best model, best parameters, and results\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Change 2: Create XGBoost model ---\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        objective='reg:squarederror',  # Standard objective for regression\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mae',\n",
        "        verbosity=0 # Use verbosity=0 instead of verbose=0\n",
        "    )\n",
        "\n",
        "    # --- Change 3: Define parameter grid (Using XGBoost names) ---\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 500],\n",
        "        'learning_rate': [0.001, 0.01, 0.1, 1],\n",
        "        'max_depth': [5, 10, 15, 20, 24],\n",
        "    }\n",
        "\n",
        "    # Calculate total fits for progress tracking\n",
        "    num_n_estimators = len(param_grid['n_estimators'])\n",
        "    num_learning_rate = len(param_grid['learning_rate'])\n",
        "    num_max_depth = len(param_grid['max_depth'])\n",
        "    total_combinations = (num_n_estimators * num_learning_rate * num_max_depth)\n",
        "    total_fits = total_combinations * cv\n",
        "\n",
        "    print(\"Performing grid search with cross-validation on training data only...\")\n",
        "    print(f\"n_estimators: {num_n_estimators} {param_grid['n_estimators']}\")\n",
        "    print(f\"learning_rate: {num_learning_rate} {param_grid['learning_rate']}\")\n",
        "    print(f\"max_depth: {num_max_depth} {param_grid['max_depth']}\")\n",
        "    print(f\"Cross-validation folds: {cv}\")\n",
        "    print(f\"Total parameter combinations: {total_combinations}\")\n",
        "    print(f\"Total model fits: {total_fits}\\n\")\n",
        "\n",
        "    # Grid search with cross-validation using MAE\n",
        "    grid_search = GridSearchCV(\n",
        "        xgb_model,\n",
        "        param_grid,\n",
        "        cv=cv,\n",
        "        scoring='neg_mean_absolute_error',\n",
        "        n_jobs=-1,\n",
        "        verbose=1 # Changed to 1 to show progress, as verbosity=0 hides all output\n",
        "    )\n",
        "\n",
        "    # Fit the grid search\n",
        "    print(\"Starting grid search...\\n\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Extract results\n",
        "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "    # Extract existing params\n",
        "    results_df['n_estimators'] = results_df['param_n_estimators']\n",
        "    results_df['learning_rate'] = results_df['param_learning_rate']\n",
        "    results_df['max_depth'] = results_df['param_max_depth']\n",
        "    results_df['mean_mae'] = -results_df['mean_test_score']\n",
        "    results_df['std_mae'] = results_df['std_test_score']\n",
        "\n",
        "    # Get best model and parameters\n",
        "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
        "    best_learning_rate = grid_search.best_params_['learning_rate']\n",
        "    best_max_depth = grid_search.best_params_['max_depth']\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_cv_mae = -grid_search.best_score_\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GRID SEARCH RESULTS (Based on Cross-Validation)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nBest n_estimators: {best_n_estimators}\")\n",
        "    print(f\"Best learning_rate: {best_learning_rate}\")\n",
        "    print(f\"Best max_depth: {best_max_depth}\")\n",
        "    print(f\"Best cross-validation MAE: {best_cv_mae:.4f}\")\n",
        "\n",
        "    print(\"\\nTop 10 combinations ranked by cross-validation MAE:\")\n",
        "    display_results = results_df[['n_estimators', 'learning_rate',\n",
        "                                  'max_depth', 'mean_mae', 'std_mae']].sort_values('mean_mae').head(10)\n",
        "    print(display_results.to_string(index=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'best_model': best_model,\n",
        "        'best_n_estimators': best_n_estimators,\n",
        "        'best_learning_rate': best_learning_rate,\n",
        "        'best_max_depth': best_max_depth,\n",
        "        'best_cv_mae': best_cv_mae,\n",
        "        'results_df': results_df,\n",
        "        'grid_search': grid_search\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "q0RTHTjXwpVD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0RTHTjXwpVD",
        "outputId": "cb9902cd-3944-4dea-a40e-1328c9b2c0fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing grid search with cross-validation on training data only...\n",
            "n_estimators: 3 [50, 100, 500]\n",
            "learning_rate: 4 [0.001, 0.01, 0.1, 1]\n",
            "max_depth: 5 [5, 10, 15, 20, 24]\n",
            "Cross-validation folds: 5\n",
            "Total parameter combinations: 60\n",
            "Total model fits: 300\n",
            "\n",
            "Starting grid search...\n",
            "\n",
            "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
            "\n",
            "======================================================================\n",
            "GRID SEARCH RESULTS (Based on Cross-Validation)\n",
            "======================================================================\n",
            "\n",
            "Best n_estimators: 500\n",
            "Best learning_rate: 0.1\n",
            "Best max_depth: 5\n",
            "Best cross-validation MAE: 0.0803\n",
            "\n",
            "Top 10 combinations ranked by cross-validation MAE:\n",
            " n_estimators  learning_rate  max_depth  mean_mae  std_mae\n",
            "          500           0.10          5  0.080318 0.006560\n",
            "          100           0.10          5  0.081196 0.006352\n",
            "          500           0.01          5  0.082355 0.006937\n",
            "           50           0.10          5  0.082684 0.006781\n",
            "          500           0.01         10  0.087400 0.009422\n",
            "          500           0.10         10  0.087594 0.009181\n",
            "          100           0.10         10  0.087606 0.009177\n",
            "           50           0.10         10  0.088086 0.009518\n",
            "          500           0.01         24  0.089037 0.008417\n",
            "          500           0.01         20  0.089047 0.008410\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "results = xgb_grid_search(X_train, y_train, cv=5)\n",
        "xgb_best_model = results['best_model']\n",
        "best_n_estimators = results['best_n_estimators']\n",
        "best_learning_rate = results['best_learning_rate']\n",
        "best_max_depth = results['best_max_depth']\n",
        "best_cv_mae = results['best_cv_mae']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "PRPj-sZ49oQi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRPj-sZ49oQi",
        "outputId": "0ba86a17-d125-4179-e00f-733024c4382f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on training and test data...\n",
            "Done!\n",
            "\n",
            "======================================================================\n",
            "FINAL MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Cross-Validation MAE: 0.0803\n",
            "Training MAE: 0.0011\n",
            "Test MAE: 0.0784\n",
            "Difference (Test - Train): 0.0772\n",
            "\n",
            "✓ Model generalization looks good\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============ Make predictions ============\n",
        "print(\"Making predictions on training and test data...\")\n",
        "y_train_pred = xgb_best_model.predict(X_train)\n",
        "y_test_pred = xgb_best_model.predict(X_test)\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# ============ Calculate MAE ============\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "# ============ Print results ============\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nCross-Validation MAE: {best_cv_mae:.4f}\")\n",
        "print(f\"Training MAE: {train_mae:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "print(f\"Difference (Test - Train): {test_mae - train_mae:.4f}\")\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "if test_mae - train_mae > 0.5:\n",
        "    print(\"\\n⚠️ Warning: Large gap between train and test MAE (possible overfitting)\")\n",
        "elif test_mae - train_mae < -0.1:\n",
        "    print(\"\\n⚠️ Warning: Test MAE is better than train MAE (check data)\")\n",
        "else:\n",
        "    print(\"\\n✓ Model generalization looks good\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sL0RMVdzyHJe",
      "metadata": {
        "id": "sL0RMVdzyHJe"
      },
      "source": [
        "Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "EI0LXCR51-9m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI0LXCR51-9m",
        "outputId": "0962f51f-a9b4-4c26-80da-9101c07b96cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Grid Search with 330 total fits (5 CV folds)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7add3d59f920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7add3d59f920> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "GRID SEARCH RESULTS\n",
            "==================================================\n",
            "Best learning_rate: 0.1\n",
            "Best nodes (units): 7\n",
            "Best batch_size: 64\n",
            "Best cross-validated MAE: 0.08154238\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers # Use models and layers as requested\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from scikeras.wrappers import KerasRegressor # CRITICAL: Use Regressor for MAE loss\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error # Use MAE for final evaluation\n",
        "\n",
        "def create_model(units, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Keras model builder for CONSTRAINED REGRESSION (using sigmoid activation\n",
        "    to force output between 0 and 1).\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(24,)),\n",
        "        layers.Dense(units=units, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile model using MAE loss\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss=\"mae\",\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Define Early Stopping Callback\n",
        "# Monitor 'loss' (validation loss) to stop overfitting\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='loss', # Monitor validation loss during CV folds\n",
        "    patience=50, # Stop if validation loss doesn't improve for 50 epochs\n",
        "    restore_best_weights=True,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# --- 2. Pipeline and Grid Search Setup ---\n",
        "\n",
        "# Create a pipeline to standardize the data and apply KerasRegressor\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    # KerasRegressor is correct for continuous targets\n",
        "    ('keras_model', KerasRegressor(\n",
        "        model=create_model,\n",
        "        callbacks=[early_stop],\n",
        "        batch_size=64,\n",
        "        epochs=100,\n",
        "        random_state=42,\n",
        "        verbose=0\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'keras_model__model__learning_rate': [0.01, 0.1], # Changed learning rate values\n",
        "    'keras_model__model__units': [2,3,4,5,6,7,8,9,10,11,12],\n",
        "    'keras_model__batch_size': [64, 128, 256],\n",
        "}\n",
        "\n",
        "# --- Fix: Define cv before it's used ---\n",
        "cv = 5\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='neg_mean_absolute_error', # GridSearchCV maximizes score\n",
        "    cv=cv, # Example CV folds\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "# --- 3. Run and Evaluate ---\n",
        "\n",
        "# Calculate the total number of fits including all parameter combinations and CV folds\n",
        "total_fits_calculated = len(param_grid['keras_model__model__learning_rate']) * len(param_grid['keras_model__model__units']) * len(param_grid['keras_model__batch_size']) * cv\n",
        "\n",
        "print(f\"Starting Grid Search with {total_fits_calculated} total fits ({cv} CV folds)...\")\n",
        "\n",
        "y_train_reshaped = y_train.values.reshape(-1, 1)\n",
        "y_test_reshaped = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Run the grid search\n",
        "grid_search = grid.fit(X_train, y_train_reshaped)\n",
        "\n",
        "# Get optimal parameters\n",
        "best_learning_rate = grid_search.best_params_['keras_model__model__learning_rate']\n",
        "best_nodes = grid_search.best_params_['keras_model__model__units']\n",
        "best_batch_size = grid_search.best_params_['keras_model__batch_size']\n",
        "best_cv_neg_mae = grid_search.best_score_\n",
        "best_cv_mae = -best_cv_neg_mae # Convert negative score back to positive MAE\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GRID SEARCH RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Best learning_rate: {best_learning_rate}\")\n",
        "print(f\"Best nodes (units): {best_nodes}\")\n",
        "print(f\"Best batch_size: {best_batch_size}\")\n",
        "print(f\"Best cross-validated MAE: {best_cv_mae:.8f}\")\n",
        "\n",
        "# Retrieve best model (the entire pipeline)\n",
        "neural_network_best_pipeline = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "4NWuFqYheL8S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NWuFqYheL8S",
        "outputId": "1876cf67-5ca8-4cdc-bbac-6b0c2b2017ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    param_keras_model__model__learning_rate  param_keras_model__model__units  param_keras_model__batch_size  mean_cv_mae  std_test_score  rank_test_score\n",
            "16                                     0.10                                7                             64     0.081542        0.008886                1\n",
            "36                                     0.10                                5                            128     0.082177        0.010538                2\n",
            "40                                     0.10                                9                            128     0.082278        0.008095                3\n",
            "38                                     0.10                                7                            128     0.083325        0.013432                4\n",
            "35                                     0.10                                4                            128     0.083528        0.010725                5\n",
            "39                                     0.10                                8                            128     0.083831        0.009875                6\n",
            "3                                      0.01                                5                             64     0.084753        0.009603                7\n",
            "57                                     0.10                                4                            256     0.084807        0.007994                8\n",
            "41                                     0.10                               10                            128     0.085173        0.010779                9\n",
            "5                                      0.01                                7                             64     0.085233        0.011823               10\n",
            "14                                     0.10                                5                             64     0.085251        0.012303               11\n",
            "15                                     0.10                                6                             64     0.085421        0.007340               12\n",
            "58                                     0.10                                5                            256     0.085615        0.008156               13\n",
            "29                                     0.01                                9                            128     0.085928        0.009628               14\n",
            "18                                     0.10                                9                             64     0.085997        0.012847               15\n",
            "37                                     0.10                                6                            128     0.086070        0.012019               16\n",
            "43                                     0.10                               12                            128     0.086109        0.012252               17\n",
            "10                                     0.01                               12                             64     0.086179        0.011341               18\n",
            "25                                     0.01                                5                            128     0.086298        0.010710               19\n",
            "17                                     0.10                                8                             64     0.086412        0.011274               20\n",
            "60                                     0.10                                7                            256     0.086520        0.006806               21\n",
            "61                                     0.10                                8                            256     0.086758        0.007727               22\n",
            "6                                      0.01                                8                             64     0.087089        0.011736               23\n",
            "28                                     0.01                                8                            128     0.087163        0.010675               24\n",
            "59                                     0.10                                6                            256     0.087266        0.008891               25\n",
            "7                                      0.01                                9                             64     0.087278        0.010931               26\n",
            "65                                     0.10                               12                            256     0.087367        0.007656               27\n",
            "62                                     0.10                                9                            256     0.087386        0.008407               28\n",
            "32                                     0.01                               12                            128     0.087645        0.012033               29\n",
            "27                                     0.01                                7                            128     0.087715        0.011593               30\n",
            "64                                     0.10                               11                            256     0.087757        0.007900               31\n",
            "8                                      0.01                               10                             64     0.087834        0.009747               32\n",
            "1                                      0.01                                3                             64     0.088142        0.010683               33\n",
            "51                                     0.01                                9                            256     0.088271        0.007908               34\n",
            "54                                     0.01                               12                            256     0.088496        0.007767               35\n",
            "34                                     0.10                                3                            128     0.088516        0.009727               36\n",
            "42                                     0.10                               11                            128     0.088539        0.009705               37\n",
            "30                                     0.01                               10                            128     0.088624        0.008180               38\n",
            "23                                     0.01                                3                            128     0.088725        0.011052               39\n",
            "4                                      0.01                                6                             64     0.088859        0.011812               40\n",
            "9                                      0.01                               11                             64     0.088873        0.007076               41\n",
            "47                                     0.01                                5                            256     0.089280        0.010283               42\n",
            "2                                      0.01                                4                             64     0.089424        0.009406               43\n",
            "24                                     0.01                                4                            128     0.089561        0.009487               44\n",
            "21                                     0.10                               12                             64     0.089857        0.011524               45\n",
            "52                                     0.01                               10                            256     0.090310        0.007112               46\n",
            "63                                     0.10                               10                            256     0.090548        0.011899               47\n",
            "19                                     0.10                               10                             64     0.090872        0.012800               48\n",
            "33                                     0.10                                2                            128     0.090966        0.013914               49\n",
            "13                                     0.10                                4                             64     0.091137        0.010076               50\n",
            "55                                     0.10                                2                            256     0.091371        0.010624               51\n",
            "50                                     0.01                                8                            256     0.091377        0.007766               52\n",
            "48                                     0.01                                6                            256     0.091715        0.010579               53\n",
            "26                                     0.01                                6                            128     0.091969        0.010631               54\n",
            "12                                     0.10                                3                             64     0.092385        0.009101               55\n",
            "46                                     0.01                                4                            256     0.092424        0.009322               56\n",
            "31                                     0.01                               11                            128     0.092480        0.007483               57\n",
            "20                                     0.10                               11                             64     0.093044        0.009531               58\n",
            "11                                     0.10                                2                             64     0.093339        0.011732               59\n",
            "45                                     0.01                                3                            256     0.093854        0.011617               60\n",
            "56                                     0.10                                3                            256     0.094318        0.008104               61\n",
            "53                                     0.01                               11                            256     0.095455        0.006991               62\n",
            "49                                     0.01                                7                            256     0.095565        0.007962               63\n",
            "0                                      0.01                                2                             64     0.099001        0.009190               64\n",
            "22                                     0.01                                2                            128     0.099320        0.009473               65\n",
            "44                                     0.01                                2                            256     0.103421        0.009311               66\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame from the results\n",
        "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Select only the most relevant columns for easy comparison\n",
        "# We convert the score back to positive MAE for easier interpretation\n",
        "cv_results_df['mean_cv_mae'] = -cv_results_df['mean_test_score']\n",
        "\n",
        "# Columns to display: Hyperparameters and the resulting MAE\n",
        "display_columns = [\n",
        "    'param_keras_model__model__learning_rate',\n",
        "    'param_keras_model__model__units',\n",
        "    'param_keras_model__batch_size',\n",
        "    'mean_cv_mae',\n",
        "    'std_test_score', # Shows the standard deviation across the 5 folds\n",
        "    'rank_test_score' # 1 is the best\n",
        "]\n",
        "\n",
        "# Sort by the MAE and print\n",
        "cv_results_sorted = cv_results_df[display_columns].sort_values(by='mean_cv_mae', ascending=True)\n",
        "\n",
        "# Print the sorted results, showing all rows due to the set_option call\n",
        "print(cv_results_sorted.to_string())\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2cv9WrH4ny99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cv9WrH4ny99",
        "outputId": "57edf269-d3fb-4bb3-c42f-5edd830fc49c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Grid Search with 135 total fits (5 CV folds)...\n",
            "\n",
            "==================================================\n",
            "GRID SEARCH RESULTS\n",
            "==================================================\n",
            "Best learning_rate: 0.01\n",
            "Best units (layer 1): 16\n",
            "Best units (layer 2): 16\n",
            "Best batch_size: 64\n",
            "Best cross-validated MAE: 0.07551464\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers # Use models and layers as requested\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from scikeras.wrappers import KerasRegressor # CRITICAL: Use Regressor for MAE loss\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error # Use MAE for final evaluation\n",
        "\n",
        "def create_model(units1, units2, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Keras model builder for CONSTRAINED REGRESSION (using sigmoid activation\n",
        "    to force output between 0 and 1).\n",
        "    Now includes two hidden layers with different unit sizes.\n",
        "    \"\"\"\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(24,)),\n",
        "        layers.Dense(units=units1, activation='relu'),\n",
        "        layers.Dense(units=units2, activation='relu'),  # Second hidden layer\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile model using MAE loss\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss=\"mae\",\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Define Early Stopping Callback\n",
        "# Monitor 'loss' (validation loss) to stop overfitting\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='loss', # Monitor validation loss during CV folds\n",
        "    patience=50, # Stop if validation loss doesn't improve for 50 epochs\n",
        "    restore_best_weights=True,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# --- 2. Pipeline and Grid Search Setup ---\n",
        "\n",
        "# Create a pipeline to standardize the data and apply KerasRegressor\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    # KerasRegressor is correct for continuous targets\n",
        "    ('keras_model', KerasRegressor(\n",
        "        model=create_model,\n",
        "        callbacks=[early_stop],\n",
        "        batch_size=64,\n",
        "        epochs=100,\n",
        "        random_state=42,\n",
        "        verbose=0\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'keras_model__model__learning_rate': [0.01],\n",
        "    'keras_model__model__units1': [16, 32, 64],  # First hidden layer\n",
        "    'keras_model__model__units2': [16, 32, 64],  # Second hidden layer\n",
        "    'keras_model__batch_size': [64, 128, 256],\n",
        "}\n",
        "\n",
        "# --- Fix: Define cv before it's used ---\n",
        "cv = 5\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    scoring='neg_mean_absolute_error', # GridSearchCV maximizes score\n",
        "    cv=cv, # Example CV folds\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "# --- 3. Run and Evaluate ---\n",
        "\n",
        "# Calculate the total number of fits including all parameter combinations and CV folds\n",
        "total_fits_calculated = len(param_grid['keras_model__model__learning_rate']) * len(param_grid['keras_model__model__units1']) * len(param_grid['keras_model__model__units2']) * len(param_grid['keras_model__batch_size']) * cv\n",
        "\n",
        "print(f\"Starting Grid Search with {total_fits_calculated} total fits ({cv} CV folds)...\")\n",
        "\n",
        "y_train_reshaped = y_train.values.reshape(-1, 1)\n",
        "y_test_reshaped = y_test.values.reshape(-1, 1)\n",
        "\n",
        "# Run the grid search\n",
        "grid_search = grid.fit(X_train, y_train_reshaped)\n",
        "\n",
        "# Get optimal parameters\n",
        "best_learning_rate = grid_search.best_params_['keras_model__model__learning_rate']\n",
        "best_units1 = grid_search.best_params_['keras_model__model__units1']\n",
        "best_units2 = grid_search.best_params_['keras_model__model__units2']\n",
        "best_batch_size = grid_search.best_params_['keras_model__batch_size']\n",
        "best_cv_neg_mae = grid_search.best_score_\n",
        "best_cv_mae = -best_cv_neg_mae # Convert negative score back to positive MAE\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"GRID SEARCH RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Best learning_rate: {best_learning_rate}\")\n",
        "print(f\"Best units (layer 1): {best_units1}\")\n",
        "print(f\"Best units (layer 2): {best_units2}\")\n",
        "print(f\"Best batch_size: {best_batch_size}\")\n",
        "print(f\"Best cross-validated MAE: {best_cv_mae:.8f}\")\n",
        "\n",
        "# Retrieve best model (the entire pipeline)\n",
        "neural_network_best_pipeline = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "ADoLQZ4ex99O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADoLQZ4ex99O",
        "outputId": "407520ca-0468-40b5-d484-d18ac441c54a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    param_keras_model__model__learning_rate  param_keras_model__model__units1  param_keras_model__model__units2  param_keras_model__batch_size  mean_cv_mae  std_test_score  rank_test_score\n",
            "0                                      0.01                                16                                16                             64     0.075515        0.001822                1\n",
            "5                                      0.01                                32                                64                             64     0.076476        0.009010                2\n",
            "7                                      0.01                                64                                32                             64     0.077228        0.006192                3\n",
            "6                                      0.01                                64                                16                             64     0.077891        0.007130                4\n",
            "11                                     0.01                                16                                64                            128     0.078041        0.004727                5\n",
            "26                                     0.01                                64                                64                            256     0.078799        0.005828                6\n",
            "17                                     0.01                                64                                64                            128     0.079123        0.007652                7\n",
            "2                                      0.01                                16                                64                             64     0.079462        0.007223                8\n",
            "24                                     0.01                                64                                16                            256     0.079780        0.008984                9\n",
            "25                                     0.01                                64                                32                            256     0.079950        0.006050               10\n",
            "15                                     0.01                                64                                16                            128     0.080027        0.005922               11\n",
            "3                                      0.01                                32                                16                             64     0.080116        0.005723               12\n",
            "20                                     0.01                                16                                64                            256     0.080474        0.005606               13\n",
            "21                                     0.01                                32                                16                            256     0.080794        0.007532               14\n",
            "9                                      0.01                                16                                16                            128     0.081382        0.005226               15\n",
            "12                                     0.01                                32                                16                            128     0.081545        0.006016               16\n",
            "14                                     0.01                                32                                64                            128     0.081665        0.010919               17\n",
            "16                                     0.01                                64                                32                            128     0.081693        0.005429               18\n",
            "13                                     0.01                                32                                32                            128     0.082360        0.005958               19\n",
            "8                                      0.01                                64                                64                             64     0.082382        0.005201               20\n",
            "4                                      0.01                                32                                32                             64     0.082384        0.005902               21\n",
            "1                                      0.01                                16                                32                             64     0.082552        0.005333               22\n",
            "22                                     0.01                                32                                32                            256     0.082631        0.005093               23\n",
            "23                                     0.01                                32                                64                            256     0.083514        0.005774               24\n",
            "19                                     0.01                                16                                32                            256     0.083989        0.007862               25\n",
            "18                                     0.01                                16                                16                            256     0.084378        0.008526               26\n",
            "10                                     0.01                                16                                32                            128     0.084461        0.005277               27\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame from the results\n",
        "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Select only the most relevant columns for easy comparison\n",
        "# We convert the score back to positive MAE for easier interpretation\n",
        "cv_results_df['mean_cv_mae'] = -cv_results_df['mean_test_score']\n",
        "\n",
        "# Columns to display: Hyperparameters and the resulting MAE\n",
        "display_columns = [\n",
        "    'param_keras_model__model__learning_rate',\n",
        "    'param_keras_model__model__units1',\n",
        "    'param_keras_model__model__units2',\n",
        "    'param_keras_model__batch_size',\n",
        "    'mean_cv_mae',\n",
        "    'std_test_score', # Shows the standard deviation across the 5 folds\n",
        "    'rank_test_score' # 1 is the best\n",
        "]\n",
        "\n",
        "# Sort by the MAE and print\n",
        "cv_results_sorted = cv_results_df[display_columns].sort_values(by='mean_cv_mae', ascending=True)\n",
        "\n",
        "# Print the sorted results, showing all rows due to the set_option call\n",
        "print(cv_results_sorted.to_string())\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "Fs0b2NF1PiqG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs0b2NF1PiqG",
        "outputId": "a37f253b-e3bf-4c32-a0f1-d4943bb6ade2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Test Set Evaluation:\n",
            "MAE on Training Set: 0.04091862\n",
            "MAE on Test Set (using best model): 0.07971519\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Use best model to predict on the validation set\n",
        "y_train_pred = neural_network_best_pipeline.predict(X_train)\n",
        "y_test_pred = neural_network_best_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Absolute Error (MAE)\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nFinal Test Set Evaluation:\")\n",
        "print(f\"MAE on Training Set: {train_mae:.8f}\")\n",
        "print(f\"MAE on Test Set (using best model): {test_mae:.8f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KU4gaKCzerIE",
      "metadata": {
        "id": "KU4gaKCzerIE"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "V1a_4BrLcANG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1a_4BrLcANG",
        "outputId": "2c5acf55-96e9-4921-b1cc-46b8c9404c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    user     label  anomtype\n",
            "0    561  0.383316         1\n",
            "1    202  0.925028         2\n",
            "2    205  0.380860         2\n",
            "3    424  0.255181         1\n",
            "4    284  0.055162         2\n",
            "5    667  0.558745         0\n",
            "6    730  0.311928         1\n",
            "7    469  0.233492         2\n",
            "8    199  0.165112         1\n",
            "9    699  0.261752         2\n",
            "10   231  0.951103         0\n",
            "11    26  0.558222         0\n",
            "12   786  0.549116         0\n",
            "13   849  0.301816         1\n",
            "14   459  0.739300         0\n"
          ]
        }
      ],
      "source": [
        "print(y_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "HAgNyzOdckFn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "HAgNyzOdckFn",
        "outputId": "a0972cb7-7d4d-49b4-ce36-ca8ff5b894ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_2ecc3 th {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "#T_2ecc3  td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_2ecc3\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_2ecc3_level0_col0\" class=\"col_heading level0 col0\" >user</th>\n",
              "      <th id=\"T_2ecc3_level0_col1\" class=\"col_heading level0 col1\" >n_ratings</th>\n",
              "      <th id=\"T_2ecc3_level0_col2\" class=\"col_heading level0 col2\" >mean</th>\n",
              "      <th id=\"T_2ecc3_level0_col3\" class=\"col_heading level0 col3\" >std</th>\n",
              "      <th id=\"T_2ecc3_level0_col4\" class=\"col_heading level0 col4\" >skew</th>\n",
              "      <th id=\"T_2ecc3_level0_col5\" class=\"col_heading level0 col5\" >n_unique_items</th>\n",
              "      <th id=\"T_2ecc3_level0_col6\" class=\"col_heading level0 col6\" >dup_rate</th>\n",
              "      <th id=\"T_2ecc3_level0_col7\" class=\"col_heading level0 col7\" >label_y</th>\n",
              "      <th id=\"T_2ecc3_level0_col8\" class=\"col_heading level0 col8\" >label_ycat</th>\n",
              "      <th id=\"T_2ecc3_level0_col9\" class=\"col_heading level0 col9\" >anomtype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row0_col0\" class=\"data row0 col0\" >26</td>\n",
              "      <td id=\"T_2ecc3_row0_col1\" class=\"data row0 col1\" >244</td>\n",
              "      <td id=\"T_2ecc3_row0_col2\" class=\"data row0 col2\" >2.922131</td>\n",
              "      <td id=\"T_2ecc3_row0_col3\" class=\"data row0 col3\" >1.185510</td>\n",
              "      <td id=\"T_2ecc3_row0_col4\" class=\"data row0 col4\" >-0.490847</td>\n",
              "      <td id=\"T_2ecc3_row0_col5\" class=\"data row0 col5\" >244</td>\n",
              "      <td id=\"T_2ecc3_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
              "      <td id=\"T_2ecc3_row0_col7\" class=\"data row0 col7\" >0.558222</td>\n",
              "      <td id=\"T_2ecc3_row0_col8\" class=\"data row0 col8\" >0.558222</td>\n",
              "      <td id=\"T_2ecc3_row0_col9\" class=\"data row0 col9\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row1_col0\" class=\"data row1 col0\" >231</td>\n",
              "      <td id=\"T_2ecc3_row1_col1\" class=\"data row1 col1\" >358</td>\n",
              "      <td id=\"T_2ecc3_row1_col2\" class=\"data row1 col2\" >2.455307</td>\n",
              "      <td id=\"T_2ecc3_row1_col3\" class=\"data row1 col3\" >1.113685</td>\n",
              "      <td id=\"T_2ecc3_row1_col4\" class=\"data row1 col4\" >0.033444</td>\n",
              "      <td id=\"T_2ecc3_row1_col5\" class=\"data row1 col5\" >358</td>\n",
              "      <td id=\"T_2ecc3_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
              "      <td id=\"T_2ecc3_row1_col7\" class=\"data row1 col7\" >0.951103</td>\n",
              "      <td id=\"T_2ecc3_row1_col8\" class=\"data row1 col8\" >0.951103</td>\n",
              "      <td id=\"T_2ecc3_row1_col9\" class=\"data row1 col9\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row2_col0\" class=\"data row2 col0\" >459</td>\n",
              "      <td id=\"T_2ecc3_row2_col1\" class=\"data row2 col1\" >230</td>\n",
              "      <td id=\"T_2ecc3_row2_col2\" class=\"data row2 col2\" >2.782609</td>\n",
              "      <td id=\"T_2ecc3_row2_col3\" class=\"data row2 col3\" >1.216930</td>\n",
              "      <td id=\"T_2ecc3_row2_col4\" class=\"data row2 col4\" >-0.177129</td>\n",
              "      <td id=\"T_2ecc3_row2_col5\" class=\"data row2 col5\" >230</td>\n",
              "      <td id=\"T_2ecc3_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
              "      <td id=\"T_2ecc3_row2_col7\" class=\"data row2 col7\" >0.739300</td>\n",
              "      <td id=\"T_2ecc3_row2_col8\" class=\"data row2 col8\" >0.739300</td>\n",
              "      <td id=\"T_2ecc3_row2_col9\" class=\"data row2 col9\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row3_col0\" class=\"data row3 col0\" >667</td>\n",
              "      <td id=\"T_2ecc3_row3_col1\" class=\"data row3 col1\" >444</td>\n",
              "      <td id=\"T_2ecc3_row3_col2\" class=\"data row3 col2\" >2.709459</td>\n",
              "      <td id=\"T_2ecc3_row3_col3\" class=\"data row3 col3\" >1.011804</td>\n",
              "      <td id=\"T_2ecc3_row3_col4\" class=\"data row3 col4\" >-0.352036</td>\n",
              "      <td id=\"T_2ecc3_row3_col5\" class=\"data row3 col5\" >444</td>\n",
              "      <td id=\"T_2ecc3_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n",
              "      <td id=\"T_2ecc3_row3_col7\" class=\"data row3 col7\" >0.558745</td>\n",
              "      <td id=\"T_2ecc3_row3_col8\" class=\"data row3 col8\" >0.558745</td>\n",
              "      <td id=\"T_2ecc3_row3_col9\" class=\"data row3 col9\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row4_col0\" class=\"data row4 col0\" >786</td>\n",
              "      <td id=\"T_2ecc3_row4_col1\" class=\"data row4 col1\" >287</td>\n",
              "      <td id=\"T_2ecc3_row4_col2\" class=\"data row4 col2\" >2.630662</td>\n",
              "      <td id=\"T_2ecc3_row4_col3\" class=\"data row4 col3\" >1.032429</td>\n",
              "      <td id=\"T_2ecc3_row4_col4\" class=\"data row4 col4\" >-0.287200</td>\n",
              "      <td id=\"T_2ecc3_row4_col5\" class=\"data row4 col5\" >287</td>\n",
              "      <td id=\"T_2ecc3_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
              "      <td id=\"T_2ecc3_row4_col7\" class=\"data row4 col7\" >0.549116</td>\n",
              "      <td id=\"T_2ecc3_row4_col8\" class=\"data row4 col8\" >0.549116</td>\n",
              "      <td id=\"T_2ecc3_row4_col9\" class=\"data row4 col9\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row5_col0\" class=\"data row5 col0\" >199</td>\n",
              "      <td id=\"T_2ecc3_row5_col1\" class=\"data row5 col1\" >211</td>\n",
              "      <td id=\"T_2ecc3_row5_col2\" class=\"data row5 col2\" >3.388626</td>\n",
              "      <td id=\"T_2ecc3_row5_col3\" class=\"data row5 col3\" >1.104391</td>\n",
              "      <td id=\"T_2ecc3_row5_col4\" class=\"data row5 col4\" >-0.603176</td>\n",
              "      <td id=\"T_2ecc3_row5_col5\" class=\"data row5 col5\" >210</td>\n",
              "      <td id=\"T_2ecc3_row5_col6\" class=\"data row5 col6\" >0.004739</td>\n",
              "      <td id=\"T_2ecc3_row5_col7\" class=\"data row5 col7\" >0.165112</td>\n",
              "      <td id=\"T_2ecc3_row5_col8\" class=\"data row5 col8\" >0.165112</td>\n",
              "      <td id=\"T_2ecc3_row5_col9\" class=\"data row5 col9\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row6_col0\" class=\"data row6 col0\" >424</td>\n",
              "      <td id=\"T_2ecc3_row6_col1\" class=\"data row6 col1\" >200</td>\n",
              "      <td id=\"T_2ecc3_row6_col2\" class=\"data row6 col2\" >3.330000</td>\n",
              "      <td id=\"T_2ecc3_row6_col3\" class=\"data row6 col3\" >0.868835</td>\n",
              "      <td id=\"T_2ecc3_row6_col4\" class=\"data row6 col4\" >-0.370751</td>\n",
              "      <td id=\"T_2ecc3_row6_col5\" class=\"data row6 col5\" >200</td>\n",
              "      <td id=\"T_2ecc3_row6_col6\" class=\"data row6 col6\" >0.000000</td>\n",
              "      <td id=\"T_2ecc3_row6_col7\" class=\"data row6 col7\" >0.255181</td>\n",
              "      <td id=\"T_2ecc3_row6_col8\" class=\"data row6 col8\" >0.255181</td>\n",
              "      <td id=\"T_2ecc3_row6_col9\" class=\"data row6 col9\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row7_col0\" class=\"data row7 col0\" >561</td>\n",
              "      <td id=\"T_2ecc3_row7_col1\" class=\"data row7 col1\" >205</td>\n",
              "      <td id=\"T_2ecc3_row7_col2\" class=\"data row7 col2\" >3.439024</td>\n",
              "      <td id=\"T_2ecc3_row7_col3\" class=\"data row7 col3\" >1.085754</td>\n",
              "      <td id=\"T_2ecc3_row7_col4\" class=\"data row7 col4\" >-0.479909</td>\n",
              "      <td id=\"T_2ecc3_row7_col5\" class=\"data row7 col5\" >201</td>\n",
              "      <td id=\"T_2ecc3_row7_col6\" class=\"data row7 col6\" >0.019512</td>\n",
              "      <td id=\"T_2ecc3_row7_col7\" class=\"data row7 col7\" >0.383316</td>\n",
              "      <td id=\"T_2ecc3_row7_col8\" class=\"data row7 col8\" >0.383316</td>\n",
              "      <td id=\"T_2ecc3_row7_col9\" class=\"data row7 col9\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row8_col0\" class=\"data row8 col0\" >730</td>\n",
              "      <td id=\"T_2ecc3_row8_col1\" class=\"data row8 col1\" >234</td>\n",
              "      <td id=\"T_2ecc3_row8_col2\" class=\"data row8 col2\" >2.923077</td>\n",
              "      <td id=\"T_2ecc3_row8_col3\" class=\"data row8 col3\" >0.850689</td>\n",
              "      <td id=\"T_2ecc3_row8_col4\" class=\"data row8 col4\" >-0.485062</td>\n",
              "      <td id=\"T_2ecc3_row8_col5\" class=\"data row8 col5\" >231</td>\n",
              "      <td id=\"T_2ecc3_row8_col6\" class=\"data row8 col6\" >0.012821</td>\n",
              "      <td id=\"T_2ecc3_row8_col7\" class=\"data row8 col7\" >0.311928</td>\n",
              "      <td id=\"T_2ecc3_row8_col8\" class=\"data row8 col8\" >0.311928</td>\n",
              "      <td id=\"T_2ecc3_row8_col9\" class=\"data row8 col9\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row9_col0\" class=\"data row9 col0\" >849</td>\n",
              "      <td id=\"T_2ecc3_row9_col1\" class=\"data row9 col1\" >212</td>\n",
              "      <td id=\"T_2ecc3_row9_col2\" class=\"data row9 col2\" >3.547170</td>\n",
              "      <td id=\"T_2ecc3_row9_col3\" class=\"data row9 col3\" >0.833335</td>\n",
              "      <td id=\"T_2ecc3_row9_col4\" class=\"data row9 col4\" >-0.671797</td>\n",
              "      <td id=\"T_2ecc3_row9_col5\" class=\"data row9 col5\" >207</td>\n",
              "      <td id=\"T_2ecc3_row9_col6\" class=\"data row9 col6\" >0.023585</td>\n",
              "      <td id=\"T_2ecc3_row9_col7\" class=\"data row9 col7\" >0.301816</td>\n",
              "      <td id=\"T_2ecc3_row9_col8\" class=\"data row9 col8\" >0.301816</td>\n",
              "      <td id=\"T_2ecc3_row9_col9\" class=\"data row9 col9\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row10_col0\" class=\"data row10 col0\" >202</td>\n",
              "      <td id=\"T_2ecc3_row10_col1\" class=\"data row10 col1\" >214</td>\n",
              "      <td id=\"T_2ecc3_row10_col2\" class=\"data row10 col2\" >4.457944</td>\n",
              "      <td id=\"T_2ecc3_row10_col3\" class=\"data row10 col3\" >0.535682</td>\n",
              "      <td id=\"T_2ecc3_row10_col4\" class=\"data row10 col4\" >-0.386071</td>\n",
              "      <td id=\"T_2ecc3_row10_col5\" class=\"data row10 col5\" >159</td>\n",
              "      <td id=\"T_2ecc3_row10_col6\" class=\"data row10 col6\" >0.257009</td>\n",
              "      <td id=\"T_2ecc3_row10_col7\" class=\"data row10 col7\" >0.925028</td>\n",
              "      <td id=\"T_2ecc3_row10_col8\" class=\"data row10 col8\" >0.925028</td>\n",
              "      <td id=\"T_2ecc3_row10_col9\" class=\"data row10 col9\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row11_col0\" class=\"data row11 col0\" >205</td>\n",
              "      <td id=\"T_2ecc3_row11_col1\" class=\"data row11 col1\" >367</td>\n",
              "      <td id=\"T_2ecc3_row11_col2\" class=\"data row11 col2\" >3.632153</td>\n",
              "      <td id=\"T_2ecc3_row11_col3\" class=\"data row11 col3\" >1.031393</td>\n",
              "      <td id=\"T_2ecc3_row11_col4\" class=\"data row11 col4\" >-0.598633</td>\n",
              "      <td id=\"T_2ecc3_row11_col5\" class=\"data row11 col5\" >342</td>\n",
              "      <td id=\"T_2ecc3_row11_col6\" class=\"data row11 col6\" >0.068120</td>\n",
              "      <td id=\"T_2ecc3_row11_col7\" class=\"data row11 col7\" >0.380860</td>\n",
              "      <td id=\"T_2ecc3_row11_col8\" class=\"data row11 col8\" >0.380860</td>\n",
              "      <td id=\"T_2ecc3_row11_col9\" class=\"data row11 col9\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row12_col0\" class=\"data row12 col0\" >284</td>\n",
              "      <td id=\"T_2ecc3_row12_col1\" class=\"data row12 col1\" >223</td>\n",
              "      <td id=\"T_2ecc3_row12_col2\" class=\"data row12 col2\" >4.107623</td>\n",
              "      <td id=\"T_2ecc3_row12_col3\" class=\"data row12 col3\" >0.575072</td>\n",
              "      <td id=\"T_2ecc3_row12_col4\" class=\"data row12 col4\" >-0.142972</td>\n",
              "      <td id=\"T_2ecc3_row12_col5\" class=\"data row12 col5\" >223</td>\n",
              "      <td id=\"T_2ecc3_row12_col6\" class=\"data row12 col6\" >0.000000</td>\n",
              "      <td id=\"T_2ecc3_row12_col7\" class=\"data row12 col7\" >0.055162</td>\n",
              "      <td id=\"T_2ecc3_row12_col8\" class=\"data row12 col8\" >0.055162</td>\n",
              "      <td id=\"T_2ecc3_row12_col9\" class=\"data row12 col9\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row13_col0\" class=\"data row13 col0\" >469</td>\n",
              "      <td id=\"T_2ecc3_row13_col1\" class=\"data row13 col1\" >269</td>\n",
              "      <td id=\"T_2ecc3_row13_col2\" class=\"data row13 col2\" >3.423792</td>\n",
              "      <td id=\"T_2ecc3_row13_col3\" class=\"data row13 col3\" >0.925767</td>\n",
              "      <td id=\"T_2ecc3_row13_col4\" class=\"data row13 col4\" >-0.258195</td>\n",
              "      <td id=\"T_2ecc3_row13_col5\" class=\"data row13 col5\" >265</td>\n",
              "      <td id=\"T_2ecc3_row13_col6\" class=\"data row13 col6\" >0.014870</td>\n",
              "      <td id=\"T_2ecc3_row13_col7\" class=\"data row13 col7\" >0.233492</td>\n",
              "      <td id=\"T_2ecc3_row13_col8\" class=\"data row13 col8\" >0.233492</td>\n",
              "      <td id=\"T_2ecc3_row13_col9\" class=\"data row13 col9\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_2ecc3_row14_col0\" class=\"data row14 col0\" >699</td>\n",
              "      <td id=\"T_2ecc3_row14_col1\" class=\"data row14 col1\" >373</td>\n",
              "      <td id=\"T_2ecc3_row14_col2\" class=\"data row14 col2\" >3.737265</td>\n",
              "      <td id=\"T_2ecc3_row14_col3\" class=\"data row14 col3\" >0.973095</td>\n",
              "      <td id=\"T_2ecc3_row14_col4\" class=\"data row14 col4\" >-0.614744</td>\n",
              "      <td id=\"T_2ecc3_row14_col5\" class=\"data row14 col5\" >359</td>\n",
              "      <td id=\"T_2ecc3_row14_col6\" class=\"data row14 col6\" >0.037534</td>\n",
              "      <td id=\"T_2ecc3_row14_col7\" class=\"data row14 col7\" >0.261752</td>\n",
              "      <td id=\"T_2ecc3_row14_col8\" class=\"data row14 col8\" >0.261752</td>\n",
              "      <td id=\"T_2ecc3_row14_col9\" class=\"data row14 col9\" >2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7adcbba3b260>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from scipy.stats import skew\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "\n",
        "# --- Compute basic user-level features ---\n",
        "def basic_features(df):\n",
        "    feats = (\n",
        "        df.groupby('user')['rating']\n",
        "        .agg(\n",
        "            n_ratings='count',\n",
        "            mean='mean',\n",
        "            std='std',\n",
        "            skew=lambda x: skew(x, bias=False) if len(x) > 2 else 0\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "    feats['n_unique_items'] = df.groupby('user')['item'].nunique().values\n",
        "    feats['dup_rate'] = 1 - feats['n_unique_items'] / feats['n_ratings']\n",
        "    return feats\n",
        "\n",
        "# --- Compute features ---\n",
        "feats = basic_features(X)\n",
        "\n",
        "# --- Merge only with users present in y_cat ---\n",
        "merged = (\n",
        "    feats\n",
        "    .merge(y, on='user', how='left')\n",
        "    .merge(y_cat, on='user', how='inner', suffixes=('_y', '_ycat'))\n",
        ")\n",
        "\n",
        "display(\n",
        "    merged\n",
        "      .sort_values(['anomtype', 'user'])\n",
        "      .style\n",
        "      .set_table_styles([{'selector': 'th, td', 'props': [('white-space', 'nowrap')]}])\n",
        "      .hide(axis='index')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "msQ5FS96cpIO",
      "metadata": {
        "id": "msQ5FS96cpIO"
      },
      "source": [
        "From a first glance, it seems very clear that users with no duplicates tend to be anomtype 0\n",
        "\n",
        "It also seems like anomtype 2 users tend to have a higher amount of ratings than anontype 1.\n",
        "\n",
        "However it is too early to make any assumptions, as this is only 15 users."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
